@InProceedings{plumb2020explaining,
  title = 	 {Explaining Groups of Points in Low-Dimensional Representations},
  author =       {Plumb, Gregory and Terhorst, Jonathan and Sankararaman, Sriram and Talwalkar, Ameet},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {7762--7771},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/plumb20a/plumb20a.pdf},
  url = 	 {
http://proceedings.mlr.press/v119/plumb20a.html
},
  abstract = 	 {A common workflow in data exploration is to learn a low-dimensional representation of the data, identify groups of points in that representation, and examine the differences between the groups to determine what they represent. We treat this workflow as an interpretable machine learning problem by leveraging the model that learned the low-dimensional representation to help identify the key differences between the groups. To solve this problem, we introduce a new type of explanation, a Global Counterfactual Explanation (GCE), and our algorithm, Transitive Global Translations (TGT), for computing GCEs. TGT identifies the differences between each pair of groups using compressed sensing but constrains those pairwise differences to be consistent among all of the groups. Empirically, we demonstrate that TGT is able to identify explanations that accurately explain the model while being relatively sparse, and that these explanations match real patterns in the data.}
}

@ARTICLE{6547979,
  author={D. E. {O'Leary}},
  journal={IEEE Intelligent Systems}, 
  title={Artificial Intelligence and Big Data}, 
  year={2013},
  volume={28},
  number={2},
  pages={96-99},
  doi={10.1109/MIS.2013.39}
}

@article{van2009dimensionality,
  abstract = {In recent years, a variety of nonlinear dimensionality reduction techniques have been
proposed that aim to address the limitations of traditional techniques such as PCA
and classical scaling. The paper presents a review and systematic comparison of
these techniques. The performances of the nonlinear techniques are investigated on
artificial and natural tasks. The results of the experiments reveal that nonlinear tech-
niques perform well on selected artificial tasks, but that this strong performance does
not necessarily extend to real-world tasks. The paper explains these results by identi-
fying weaknesses of current nonlinear techniques, and suggests how the performance
of nonlinear dimensionality reduction techniques may be improved.

Real-world data, such as speech signals, digital photographs, or fMRI scans, usually has a high dimen-
sionality. In order to handle such real-world data adequately, its dimensionality needs to be reduced.
Dimensionality reduction is the transformation of high-dimensional data into a meaningful representa-
tion of reduced dimensionality. Ideally, the reduced representation should have a dimensionality that
corresponds to the intrinsic dimensionality of the data. The intrinsic dimensionality of data is the mini-
mum number of parameters needed to account for the observed properties of the data [49]. Dimension-
ality reduction is important in many domains, since it mitigates the curse of dimensionality and other
undesired properties of high-dimensional spaces [69]. As a result, dimensionality reduction facilitates,
among others, classification, visualization, and compression of high-dimensional data. Traditionally, di-
mensionality reduction was performed using linear techniques such as Principal Components Analysis
(PCA) [98], factor analysis [117], and classical scaling [126]. However, these linear techniques cannot
adequately handle complex nonlinear data.
In the last decade, a large number of nonlinear techniques for dimensionality reduction have been
proposed. See for an overview, e.g., [26, 110, 83, 131]. In contrast to the traditional linear techniques,
the nonlinear techniques have the ability to deal with complex nonlinear data. In particular for real-
world data, the nonlinear dimensionality reduction techniques may offer an advantage, because real-
world data is likely to form a highly nonlinear manifold. Previous studies have shown that nonlinear
techniques outperform their linear counterparts on complex artificial tasks. For instance, the Swiss roll
dataset comprises a set of points that lie on a spiral-like two-dimensional manifold that is embedded
within a three-dimensional space. A vast number of nonlinear techniques are perfectly able to find this
embedding, whereas linear techniques fail to do so. In contrast to these successes on artificial datasets,
successful applications of nonlinear dimensionality reduction techniques on natural datasets are less
convincing. Beyond this observation, it is not clear to what extent the performances of the various
dimensionality reduction techniques differ on artificial and natural tasks (a comparison is performed in
[94], but this comparison is very limited in scope with respect to the number of techniques and tasks
that are addressed).
Motivated by the lack of a systematic comparison of dimensionality reduction techniques, this paper
presents a comparative study of the most important linear dimensionality reduction technique (PCA),
and twelve frontranked nonlinear dimensionality reduction techniques. The aims of the paper are (1)
to investigate to what extent novel nonlinear dimensionality reduction techniques outperform the tradi-
tional PCA on real-world datasets and (2) to identify the inherent weaknesses of the twelve nonlinear
dimensionality reduction techniques. The investigation is performed by both a theoretical and an empir-
ical evaluation of the dimensionality reduction techniques. The identification is performed by a careful
analysis of the empirical results on specifically designed artificial datasets and on a selection of real-
world datasets.
Next to PCA, the paper investigates the following twelve nonlinear techniques: (1) Kernel PCA,
(2) Isomap, (3) Maximum Variance Unfolding, (4) diffusion maps, (5) Locally Linear Embedding,
(6) Laplacian Eigenmaps, (7) Hessian LLE, (8) Local Tangent Space Analysis, (9) Sammon mapping,
(10) multilayer autoencoders, (11) Locally Linear Coordination, and (12) manifold charting.},
  added-at = {2016-04-14T01:21:52.000+0200},
  author = {Van Der Maaten, Laurens and Postma, Eric and Van den Herik, Jaap},
  biburl = {https://www.bibsonomy.org/bibtex/2ed03568f0e9bca9cdaf6b25304e55940/peter.ralph},
  interhash = {f1c39ec766293d0203a327a6dd5d9948},
  intrahash = {ed03568f0e9bca9cdaf6b25304e55940},
  journal = {J Mach Learn Res},
  keywords = {PCA data_analysis machine_learning methods review visualization},
  pages = {66-71},
  timestamp = {2016-04-14T01:23:22.000+0200},
  title = {Dimensionality reduction: a comparative review},
  volume = 10,
  year = 2009
}

@article{ding2018interpretable,
	title = {Interpretable dimensionality reduction of single cell transcriptome data with deep generative models},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-04368-5},
	doi = {10.1038/s41467-018-04368-5},
	abstract = {Single-cell RNA-sequencing has great potential to discover cell types, identify cell states, trace development lineages, and reconstruct the spatial organization of cells. However, dimension reduction to interpret structure in single-cell sequencing data remains a challenge. Existing algorithms are either not able to uncover the clustering structures in the data or lose global information such as groups of clusters that are close to each other. We present a robust statistical model, scvis, to capture and visualize the low-dimensional structures in single-cell gene expression data. Simulation results demonstrate that low-dimensional representations learned by scvis preserve both the local and global neighbor structures in the data. In addition, scvis is robust to the number of data points and learns a probabilistic parametric mapping function to add new data points to an existing embedding. We then use scvis to analyze four single-cell RNA-sequencing datasets, exemplifying interpretable two-dimensional representations of the high-dimensional single-cell RNA-sequencing data.},
	language = {en},
	number = {1},
	urldate = {2021-04-13},
	journal = {Nature Communications},
	author = {Ding, Jiarui and Condon, Anne and Shah, Sohrab P.},
	month = may,
	year = {2018},
	pages = {2002},
}

@article{he2019mlframeworks,
author = {He, Horace},
title = {The State of Machine Learning Frameworks in 2019},
journal = {The Gradient},
year = {2019},
howpublished = {\url{https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/ } },
}

@article{shekhar2016comprehensive,
	title = {Comprehensive {Classification} of {Retinal} {Bipolar} {Neurons} by {Single}-{Cell} {Transcriptomics}},
	volume = {166},
	issn = {1097-4172},
	doi = {10.1016/j.cell.2016.07.054},
	abstract = {Patterns of gene expression can be used to characterize and classify neuronal types. It is challenging, however, to generate taxonomies that fulfill the essential criteria of being comprehensive, harmonizing with conventional classification schemes, and lacking superfluous subdivisions of genuine types. To address these challenges, we used massively parallel single-cell RNA profiling and optimized computational methods on a heterogeneous class of neurons, mouse retinal bipolar cells (BCs). From a population of ∼25,000 BCs, we derived a molecular classification that identified 15 types, including all types observed previously and two novel types, one of which has a non-canonical morphology and position. We validated the classification scheme and identified dozens of novel markers using methods that match molecular expression to cell morphology. This work provides a systematic methodology for achieving comprehensive molecular classification of neurons, identifies novel neuronal types, and uncovers transcriptional differences that distinguish types within a class.},
	language = {eng},
	number = {5},
	journal = {Cell},
	author = {Shekhar, Karthik and Lapan, Sylvain W. and Whitney, Irene E. and Tran, Nicholas M. and Macosko, Evan Z. and Kowalczyk, Monika and Adiconis, Xian and Levin, Joshua Z. and Nemesh, James and Goldman, Melissa and McCarroll, Steven A. and Cepko, Constance L. and Regev, Aviv and Sanes, Joshua R.},
	month = aug,
	year = {2016},
	pmid = {27565351},
	pmcid = {PMC5003425},
	keywords = {Amacrine Cells, Animals, Cluster Analysis, Female, Genetic Markers, Male, Mice, Mice, Inbred Strains, Mice, Transgenic, Retinal Bipolar Cells, Sequence Analysis, RNA, Single-Cell Analysis, Transcription, Genetic, Transcriptome},
	pages = {1308--1323.e30},
}

@article{MANCISIDOR2021114020,
title = {Learning latent representations of bank customers with the Variational Autoencoder},
journal = {Expert Systems with Applications},
volume = {164},
pages = {114020},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114020},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420307910},
author = {Rogelio A. Mancisidor and Michael Kampffmeyer and Kjersti Aas and Robert Jenssen},
keywords = {Variational Autoencoder, Data representations, Clustering, Machine learning},
abstract = {Learning data representations that reflect the customers’ creditworthiness can improve marketing campaigns, customer relationship management, data and process management or the credit risk assessment in retail banks. In this research, we show that it is possible to steer data representations in the latent space of the Variational Autoencoder (VAE) using a semi-supervised learning framework and a specific grouping of the input data called Weight of Evidence (WoE). Our proposed method learns a latent representation of the data showing a well-defied clustering structure. The clustering structure captures the customers’ creditworthiness, which is unknown a priori and cannot be identified in the input space. The main advantages of our proposed method are that it captures the natural clustering of the data, suggests the number of clusters, captures the spatial coherence of customers’ creditworthiness, generates data representations of unseen customers and assign them to one of the existing clusters. Our empirical results, based on real data sets reflecting different market and economic conditions, show that none of the well-known data representation models in the benchmark analysis are able to obtain well-defined clustering structures like our proposed method. Further, we show how banks can use our proposed methodology to improve marketing campaigns and credit risk assessment.}
}

@article{cayton2005algorithms,
  title={Algorithms for manifold learning},
  author={Cayton, Lawrence},
  journal={Univ. of California at San Diego Tech. Rep},
  volume={12},
  number={1-17},
  pages={1},
  year={2005}
}

@article{doi:10.1198/106186006X113430,
author = {Hui Zou and Trevor Hastie and Robert Tibshirani},
title = {Sparse Principal Component Analysis},
journal = {Journal of Computational and Graphical Statistics},
volume = {15},
number = {2},
pages = {265-286},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/106186006X113430},
URL = {https://doi.org/10.1198/106186006X113430},
eprint = {https://doi.org/10.1198/106186006X113430}
}

@InProceedings{10.1007/BFb0020217,
author="Sch{\"o}lkopf, Bernhard
and Smola, Alexander
and M{\"u}ller, Klaus-Robert",
editor="Gerstner, Wulfram
and Germond, Alain
and Hasler, Martin
and Nicoud, Jean-Daniel",
title="Kernel principal component analysis",
booktitle="Artificial Neural Networks --- ICANN'97",
year="1997",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="583--588",
abstract="A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.",
isbn="978-3-540-69620-9"
}

@article {Roweis2323,
	author = {Roweis, Sam T. and Saul, Lawrence K.},
	title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
	volume = {290},
	number = {5500},
	pages = {2323--2326},
	year = {2000},
	doi = {10.1126/science.290.5500.2323},
	publisher = {American Association for the Advancement of Science},
	abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/290/5500/2323},
	eprint = {https://science.sciencemag.org/content/290/5500/2323.full.pdf},
	journal = {Science}
}

@software{damiaan_j_w_reijnaers_2021_4686025,
  author       = {Damiaan J. W. Reijnaers and
                  Daniël B. van de Pavert and
                  Giguru Scheuer},
  title        = {{damiaanr/fact-ai: Reproduction of (Plumb et al., 
                   2020)}},
  month        = apr,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {1.0.0},
  doi          = {10.5281/zenodo.4686025},
  url          = {https://doi.org/10.5281/zenodo.4686025}
}