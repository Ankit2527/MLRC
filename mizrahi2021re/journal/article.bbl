% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{bengio1994learning}{article}{}
      \name{author}{3}{}{%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=c866c0bcd8215f257f24444a7f53932f}{%
           family={Simard},
           familyi={S\bibinitperiod},
           given={Patrice},
           giveni={P\bibinitperiod}}}%
        {{hash=f247df2d5de3a9ccb24aa9ee9fdaf277}{%
           family={Frasconi},
           familyi={F\bibinitperiod},
           given={Paolo},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{e394d4a496ef63d7c69ef669365d2b97}
      \strng{fullhash}{e394d4a496ef63d7c69ef669365d2b97}
      \strng{bibnamehash}{e394d4a496ef63d7c69ef669365d2b97}
      \strng{authorbibnamehash}{e394d4a496ef63d7c69ef669365d2b97}
      \strng{authornamehash}{e394d4a496ef63d7c69ef669365d2b97}
      \strng{authorfullhash}{e394d4a496ef63d7c69ef669365d2b97}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{IEEE transactions on neural networks}
      \field{number}{2}
      \field{title}{Learning long-term dependencies with gradient descent is difficult}
      \field{volume}{5}
      \field{year}{1994}
      \field{pages}{157\bibrangedash 166}
      \range{pages}{10}
    \endentry
    \entry{zhang2019analysis}{article}{}
      \name{author}{4}{}{%
        {{hash=854808860a9d7b07f59f160fcdd2643c}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Jingzhao},
           giveni={J\bibinitperiod}}}%
        {{hash=da1d01d5f3b429497ebd443f6ab849c1}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Tianxing},
           giveni={T\bibinitperiod}}}%
        {{hash=533357302a33eccdc40c943cd97d2edc}{%
           family={Sra},
           familyi={S\bibinitperiod},
           given={Suvrit},
           giveni={S\bibinitperiod}}}%
        {{hash=9814576d57251b512116243e61b38a6e}{%
           family={Jadbabaie},
           familyi={J\bibinitperiod},
           given={Ali},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{b610cc56db282d2cf606b9cdc14cb835}
      \strng{fullhash}{774d38ef7ccaf8160896c3d6d5cf1aa5}
      \strng{bibnamehash}{774d38ef7ccaf8160896c3d6d5cf1aa5}
      \strng{authorbibnamehash}{774d38ef7ccaf8160896c3d6d5cf1aa5}
      \strng{authornamehash}{b610cc56db282d2cf606b9cdc14cb835}
      \strng{authorfullhash}{774d38ef7ccaf8160896c3d6d5cf1aa5}
      \field{extraname}{1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{arXiv preprint arXiv:1905.11881}
      \field{title}{Analysis of Gradient Clipping and Adaptive Scaling with a Relaxed Smoothness Condition.}
      \field{year}{2019}
    \endentry
    \entry{menon_can_2019}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=d5918aa4a3a43984bc6bd2bf025e1869}{%
           family={Menon},
           familyi={M\bibinitperiod},
           given={Aditya\bibnamedelima Krishna},
           giveni={A\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=8591482d0b6f9a103131dc09cc958846}{%
           family={Rawat},
           familyi={R\bibinitperiod},
           given={Ankit\bibnamedelima Singh},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=3b0b77d59993d2550f1dac0336347c8a}{%
           family={Kumar},
           familyi={K\bibinitperiod},
           given={Sanjiv},
           giveni={S\bibinitperiod}}}%
        {{hash=0c4d24d69fe276be772b20facd310544}{%
           family={Reddi},
           familyi={R\bibinitperiod},
           given={Sashank},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{5c7593bd4c393505aea0fa43edf74a06}
      \strng{fullhash}{123399090d82b07f5e6caa558ad044f7}
      \strng{bibnamehash}{123399090d82b07f5e6caa558ad044f7}
      \strng{authorbibnamehash}{123399090d82b07f5e6caa558ad044f7}
      \strng{authornamehash}{5c7593bd4c393505aea0fa43edf74a06}
      \strng{authorfullhash}{123399090d82b07f5e6caa558ad044f7}
      \field{extraname}{1}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{International Conference on Learning Representations (ICLR)}
      \field{title}{Can gradient clipping mitigate label noise?}
      \field{year}{2020}
    \endentry
    \entry{ekholm1982model}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=ffbbbf56955c2d4fe00a3d4e5807902b}{%
           family={Ekholm},
           familyi={E\bibinitperiod},
           given={Anders},
           giveni={A\bibinitperiod}}}%
        {{hash=352ec014f45820c2eefd65fb0d4a9aff}{%
           family={Palmgren},
           familyi={P\bibinitperiod},
           given={Juni},
           giveni={J\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {Springer}%
      }
      \strng{namehash}{c3b487455297ad923ae5245040636204}
      \strng{fullhash}{c3b487455297ad923ae5245040636204}
      \strng{bibnamehash}{c3b487455297ad923ae5245040636204}
      \strng{authorbibnamehash}{c3b487455297ad923ae5245040636204}
      \strng{authornamehash}{c3b487455297ad923ae5245040636204}
      \strng{authorfullhash}{c3b487455297ad923ae5245040636204}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{GLIM 82: Proceedings of the international conference on generalised linear models}
      \field{title}{A model for a binary response with misclassifications}
      \field{year}{1982}
      \field{pages}{128\bibrangedash 143}
      \range{pages}{16}
    \endentry
    \entry{menon2015learning}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=89e45dba1abc46b5b4aa4e462a3156e7}{%
           family={Menon},
           familyi={M\bibinitperiod},
           given={Aditya},
           giveni={A\bibinitperiod}}}%
        {{hash=86166bda2172a6f9d8a706889541150b}{%
           family={Van\bibnamedelima Rooyen},
           familyi={V\bibinitperiod\bibinitdelim R\bibinitperiod},
           given={Brendan},
           giveni={B\bibinitperiod}}}%
        {{hash=25aead4b15ea1d822aac07a7de816978}{%
           family={Ong},
           familyi={O\bibinitperiod},
           given={Cheng\bibnamedelima Soon},
           giveni={C\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=48bdf09f541745e66b25a93b8f0f734d}{%
           family={Williamson},
           familyi={W\bibinitperiod},
           given={Bob},
           giveni={B\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {PMLR}%
      }
      \strng{namehash}{56db8f0a6cacfb731324f07b9307a2f5}
      \strng{fullhash}{f2401be5d5c3c91336be3052053428f2}
      \strng{bibnamehash}{f2401be5d5c3c91336be3052053428f2}
      \strng{authorbibnamehash}{f2401be5d5c3c91336be3052053428f2}
      \strng{authornamehash}{56db8f0a6cacfb731324f07b9307a2f5}
      \strng{authorfullhash}{f2401be5d5c3c91336be3052053428f2}
      \field{extraname}{2}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{International Conference on Machine Learning}
      \field{title}{Learning from corrupted binary labels via class-probability estimation}
      \field{year}{2015}
      \field{pages}{125\bibrangedash 134}
      \range{pages}{10}
    \endentry
    \entry{zhang_generalized_2018}{article}{}
      \name{author}{2}{}{%
        {{hash=35e75c41b2ad52e82ab5690b269900bb}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Zhilu},
           giveni={Z\bibinitperiod}}}%
        {{hash=885ad74f7e4e07ac204c4f7ea0af8dee}{%
           family={Sabuncu},
           familyi={S\bibinitperiod},
           given={Mert\bibnamedelima R.},
           giveni={M\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
      }
      \strng{namehash}{3816823a5cd71986a8cf0a52f9a55e29}
      \strng{fullhash}{3816823a5cd71986a8cf0a52f9a55e29}
      \strng{bibnamehash}{3816823a5cd71986a8cf0a52f9a55e29}
      \strng{authorbibnamehash}{3816823a5cd71986a8cf0a52f9a55e29}
      \strng{authornamehash}{3816823a5cd71986a8cf0a52f9a55e29}
      \strng{authorfullhash}{3816823a5cd71986a8cf0a52f9a55e29}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.}
      \field{annotation}{Comment: 32nd Conference on Neural Information Processing Systems (NeurIPS 2018)}
      \field{journaltitle}{arXiv:1805.07836 [cs, stat]}
      \field{month}{11}
      \field{note}{arXiv: 1805.07836}
      \field{title}{Generalized {Cross} {Entropy} {Loss} for {Training} {Deep} {Neural} {Networks} with {Noisy} {Labels}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2018}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/dmizrahi/Zotero/storage/D9IAM4U2/Zhang and Sabuncu - 2018 - Generalized Cross Entropy Loss for Training Deep N.pdf:application/pdf;arXiv.org Snapshot:/Users/dmizrahi/Zotero/storage/488R72AY/1805.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1805.07836
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1805.07836
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{van_rooyen_learning_2015}{article}{}
      \name{author}{3}{}{%
        {{hash=9f578d28e73975d36afebf0eb909e756}{%
           family={Rooyen},
           familyi={R\bibinitperiod},
           given={Brendan},
           giveni={B\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
        {{hash=d5918aa4a3a43984bc6bd2bf025e1869}{%
           family={Menon},
           familyi={M\bibinitperiod},
           given={Aditya\bibnamedelima Krishna},
           giveni={A\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=2e35ef7841aac7d181dcf0a232a11d6d}{%
           family={Williamson},
           familyi={W\bibinitperiod},
           given={Robert\bibnamedelima C.},
           giveni={R\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{a714cb6711d43211bb594ca99f2d9a1b}
      \strng{fullhash}{a714cb6711d43211bb594ca99f2d9a1b}
      \strng{bibnamehash}{a714cb6711d43211bb594ca99f2d9a1b}
      \strng{authorbibnamehash}{a714cb6711d43211bb594ca99f2d9a1b}
      \strng{authornamehash}{a714cb6711d43211bb594ca99f2d9a1b}
      \strng{authorfullhash}{a714cb6711d43211bb594ca99f2d9a1b}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the SLN-robustness of the unhinged loss.}
      \field{journaltitle}{arXiv:1505.07634 [cs]}
      \field{month}{5}
      \field{note}{arXiv: 1505.07634}
      \field{shorttitle}{Learning with {Symmetric} {Label} {Noise}}
      \field{title}{Learning with {Symmetric} {Label} {Noise}: {The} {Importance} of {Being} {Unhinged}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2015}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/dmizrahi/Zotero/storage/HIYNUEAL/van Rooyen et al. - 2015 - Learning with Symmetric Label Noise The Importanc.pdf:application/pdf;arXiv.org Snapshot:/Users/dmizrahi/Zotero/storage/7TD7UQYG/1505.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1505.07634
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1505.07634
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{huber_robust_1964}{article}{}
      \name{author}{1}{}{%
        {{hash=2e7489c001c33b64b64d65458065bd5c}{%
           family={Huber},
           familyi={H\bibinitperiod},
           given={Peter\bibnamedelima J.},
           giveni={P\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{2e7489c001c33b64b64d65458065bd5c}
      \strng{fullhash}{2e7489c001c33b64b64d65458065bd5c}
      \strng{bibnamehash}{2e7489c001c33b64b64d65458065bd5c}
      \strng{authorbibnamehash}{2e7489c001c33b64b64d65458065bd5c}
      \strng{authornamehash}{2e7489c001c33b64b64d65458065bd5c}
      \strng{authorfullhash}{2e7489c001c33b64b64d65458065bd5c}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper contains a new approach toward a theory of robust estimation; it treats in detail the asymptotic theory of estimating a location parameter for contaminated normal distributions, and exhibits estimators--intermediaries between sample mean and sample median--that are asymptotically most robust (in a sense to be specified) among all translation invariant estimators. For the general background, see Tukey (1960) (p. 448 ff.) Let x1, ⋯, xn be independent random variables with common distribution function F(t - ξ). The problem is to estimate the location parameter ξ, but with the complication that the prototype distribution F(t) is only approximately known. I shall primarily be concerned with the model of indeterminacy F = (1 - ε)Φ + ε H, where \$0 {\textbackslash}leqq {\textbackslash}epsilon {<} 1\$ is a known number, Φ(t) = (2π)-1/2 ∫t -∞ exp(-1/2s2) ds is the standard normal cumulative and H is an unknown contaminating distribution. This model arises for instance if the observations are assumed to be normal with variance 1, but a fraction ε of them is affected by gross errors. Later on, I shall also consider other models of indeterminacy, e.g., \${\textbackslash}sup\_t {|}F(t) - {\textbackslash}Phi(t){|} {\textbackslash}leqq {\textbackslash}epsilon\$. Some inconvenience is caused by the fact that location and scale parameters are not uniquely determined: in general, for fixed ε, there will be several values of ξ and σ such that \${\textbackslash}sup\_t{|}F(t) - {\textbackslash}Phi((t - {\textbackslash}xi)/{\textbackslash}sigma){|} {\textbackslash}leqq {\textbackslash}epsilon\$, and similarly for the contaminated case. Although this inherent and unavoidable indeterminacy is small if ε is small and is rather irrelevant for practical purposes, it poses awkward problems for the theory, especially for optimality questions. To remove this difficulty, one may either (i) restrict attention to symmetric distributions, and estimate the location of the center of symmetry (this works for ξ but not for σ); or (ii) one may define the parameter to be estimated in terms of the estimator itself, namely by its asymptotic value for sample size n → ∞; or (iii) one may define the parameters by arbitrarily chosen functionals of the distribution (e.g., by the expectation, or the median of F). All three possibilities have unsatisfactory aspects, and I shall usually choose the variant which is mathematically most convenient. It is interesting to look back to the very origin of the theory of estimation, namely to Gauss and his theory of least squares. Gauss was fully aware that his main reason for assuming an underlying normal distribution and a quadratic loss function was mathematical, i.e., computational, convenience. In later times, this was often forgotten, partly because of the central limit theorem. However, if one wants to be honest, the central limit theorem can at most explain why many distributions occurring in practice are approximately normal. The stress is on the word "approximately." This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): What happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance: seemingly quite mild deviations may already explode its variance. Tukey and others proposed several more robust substitutes--trimmed means, Winsorized means, etc.--and explored their performance for a few typical violations of normality. A general theory of robust estimation is still lacking; it is hoped that the present paper will furnish the first few steps toward such a theory. At the core of the method of least squares lies the idea to minimize the sum of the squared "errors," that is, to adjust the unknown parameters such that the sum of the squares of the differences between observed and computed values is minimized. In the simplest case, with which we are concerned here, namely the estimation of a location parameter, one has to minimize the expression ∑i (xi - T)2; this is of course achieved by the sample mean T = ∑i xi/n. I should like to emphasize that no loss function is involved here; I am only describing how the least squares estimator is defined, and neither the underlying family of distributions nor the true value of the parameter to be estimated enters so far. It is quite natural to ask whether one can obtain more robustness by minimizing another function of the errors than the sum of their squares. We shall therefore concentrate our attention to estimators that can be defined by a minimum principle of the form (for a location parameter): T = Tn(x1, ⋯, xn) minimizes ∑i ρ(xi - T), {\textbackslash}begin\{equation*\} {\textbackslash}tag\{M\} where {\textbackslash}rho is a non-constant function. {\textbackslash}end\{equation*\} Of course, this definition generalizes at once to more general least squares type problems, where several parameters have to be determined. This class of estimators contains in particular (i) the sample mean (ρ(t) = t2), (ii) the sample median (ρ(t) = {|}t{|}), and more generally, (iii) all maximum likelihood estimators (ρ(t) = -log f(t), where f is the assumed density of the untranslated distribution). These (M)-estimators, as I shall call them for short, have rather pleasant asymptotic properties; sufficient conditions for asymptotic normality and an explicit expression for their asymptotic variance will be given. How should one judge the robustness of an estimator Tn(x) = Tn(x1, ⋯, xn)? Since ill effects from contamination are mainly felt for large sample sizes, it seems that one should primarily optimize large sample robustness properties. Therefore, a convenient measure of robustness for asymptotically normal estimators seems to be the supremum of the asymptotic variance (n → ∞) when F ranges over some suitable set of underlying distributions, in particular over the set of all F = (1 - ε)Φ + ε H for fixed ε and symmetric H. On second thought, it turns out that the asymptotic variance is not only easier to handle, but that even for moderate values of n it is a better measure of performance than the actual variance, because (i) the actual variance of an estimator depends very much on the behavior of the tails of H, and the supremum of the actual variance is infinite for any estimator whose value is always contained in the convex hull of the observations. (ii) If an estimator is asymptotically normal, then the important central part of its distribution and confidence intervals for moderate confidence levels can better be approximated in terms of the asymptotic variance than in terms of the actual variance. If we adopt this measure of robustness, and if we restrict attention to (M)-estimators, then it will be shown that the most robust estimator is uniquely determined and corresponds to the following ρ:ρ(t) = 1/2t2 for \${|}t{|} {<} k, {\textbackslash}rho(t) = k{|}t{|} - {\textbackslash}frac\{1\}\{2\}k{\textasciicircum}2\$ for {|}t{|} ≥ k, with k depending on ε. This estimator is most robust even among all translation invariant estimators. Sample mean (k = ∞) and sample median (k = 0) are limiting cases corresponding to ε = 0 and ε = 1, respectively, and the estimator is closely related and asymptotically equivalent to Winsorizing. I recall the definition of Winsorizing: assume that the observations have been ordered, x1 ≤ x2 ≤ ⋯ ≤ xn, then the statistic T = n-1(gxg + 1 + xg + 1 + xg + 2 + ⋯ + xn - h + hxn - h) is called the Winsorized mean, obtained by Winsorizing the g leftmost and the h rightmost observations. The above most robust (M)-estimators can be described by the same formula, except that in the first and in the last summand, the factors xg + 1 and xn - h have to be replaced by some numbers u, v satisfying xg ≤ u ≤ xg + 1 and xn - h ≤ v ≤ xn - h + 1, respectively; g, h, u and v depend on the sample. In fact, this (M)-estimator is the maximum likelihood estimator corresponding to a unique least favorable distribution F0 with density f0(t) = (1 - ε)(2π)-1/2e-ρ(t). This f0 behaves like a normal density for small t, like an exponential density for large t. At least for me, this was rather surprising--I would have expected an f0 with much heavier tails. This result is a particular case of a more general one that can be stated roughly as follows: Assume that F belongs to some convex set C of distribution functions. Then the most robust (M)-estimator for the set C coincides with the maximum likelihood estimator for the unique F0 ε C which has the smallest Fisher information number I(F) = ∫ (f'/f)2f dt among all F ε C. Miscellaneous related problems will also be treated: the case of non-symmetric contaminating distributions; the most robust estimator for the model of indeterminacy \${\textbackslash}sup\_t{|}F(t) - {\textbackslash}Phi(t){|} {\textbackslash}leqq {\textbackslash}epsilon\$; robust estimation of a scale parameter; how to estimate location, if scale and ε are unknown; numerical computation of the estimators; more general estimators, e.g., minimizing \${\textbackslash}sum\_\{i {<} j\} {\textbackslash}rho(x\_i - T, x\_j - T)\$, where ρ is a function of two arguments. Questions of small sample size theory will not be touched in this paper.}
      \field{issn}{0003-4851}
      \field{journaltitle}{The Annals of Mathematical Statistics}
      \field{note}{Publisher: Institute of Mathematical Statistics}
      \field{number}{1}
      \field{title}{Robust {Estimation} of a {Location} {Parameter}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{volume}{35}
      \field{year}{1964}
      \field{urldateera}{ce}
      \field{pages}{73\bibrangedash 101}
      \range{pages}{29}
      \verb{urlraw}
      \verb https://www.jstor.org/stable/2238020
      \endverb
      \verb{url}
      \verb https://www.jstor.org/stable/2238020
      \endverb
    \endentry
    \entry{long_random_2010}{article}{}
      \name{author}{2}{}{%
        {{hash=f3dcb00987c966c735accd4d34388f3c}{%
           family={Long},
           familyi={L\bibinitperiod},
           given={Philip\bibnamedelima M.},
           giveni={P\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=7e130dd65380968821f792904ac745c9}{%
           family={Servedio},
           familyi={S\bibinitperiod},
           given={Rocco\bibnamedelima A.},
           giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{23dea3b450371c967a364ed9a0a16f2c}
      \strng{fullhash}{23dea3b450371c967a364ed9a0a16f2c}
      \strng{bibnamehash}{23dea3b450371c967a364ed9a0a16f2c}
      \strng{authorbibnamehash}{23dea3b450371c967a364ed9a0a16f2c}
      \strng{authornamehash}{23dea3b450371c967a364ed9a0a16f2c}
      \strng{authorfullhash}{23dea3b450371c967a364ed9a0a16f2c}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A broad class of boosting algorithms can be interpreted as performing coordinate-wise gradient descent to minimize some potential function of the margins of a data set. This class includes AdaBoost, LogitBoost, and other widely used and well-studied boosters. In this paper we show that for a broad class of convex potential functions, any such boosting algorithm is highly susceptible to random classification noise. We do this by showing that for any such booster and any nonzero random classification noise rate η, there is a simple data set of examples which is efficiently learnable by such a booster if there is no noise, but which cannot be learned to accuracy better than 1/2 if there is random classification noise at rate η. This holds even if the booster regularizes using early stopping or a bound on the L1 norm of the voting weights. This negative result is in contrast with known branching program based boosters which do not fall into the convex potential function framework and which can provably learn to high accuracy in the presence of random classification noise.}
      \field{issn}{1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{month}{3}
      \field{number}{3}
      \field{title}{Random classification noise defeats all convex potential boosters}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{volume}{78}
      \field{year}{2010}
      \field{urldateera}{ce}
      \field{pages}{287\bibrangedash 304}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1007/s10994-009-5165-z
      \endverb
      \verb{file}
      \verb Springer Full Text PDF:/Users/dmizrahi/Zotero/storage/KFGV49KD/Long and Servedio - 2010 - Random classification noise defeats all convex pot.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10994-009-5165-z
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10994-009-5165-z
      \endverb
    \endentry
    \entry{ding_statistical_2013}{thesis}{}
      \name{author}{1}{}{%
        {{hash=cdcad02b1e1e277b45499790b7ccacde}{%
           family={Ding},
           familyi={D\bibinitperiod},
           given={Nan},
           giveni={N\bibinitperiod}}}%
      }
      \strng{namehash}{cdcad02b1e1e277b45499790b7ccacde}
      \strng{fullhash}{cdcad02b1e1e277b45499790b7ccacde}
      \strng{bibnamehash}{cdcad02b1e1e277b45499790b7ccacde}
      \strng{authorbibnamehash}{cdcad02b1e1e277b45499790b7ccacde}
      \strng{authornamehash}{cdcad02b1e1e277b45499790b7ccacde}
      \strng{authorfullhash}{cdcad02b1e1e277b45499790b7ccacde}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{month}{1}
      \field{title}{Statistical machine learning in the t-exponential family of distributions}
      \field{type}{phdthesis}
      \field{year}{2013}
      \verb{file}
      \verb :/Users/dmizrahi/Zotero/storage/ZNQ5NVRB/AAI3591196.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://docs.lib.purdue.edu/dissertations/AAI3591196
      \endverb
      \verb{url}
      \verb https://docs.lib.purdue.edu/dissertations/AAI3591196
      \endverb
    \endentry
    \entry{harris_array_2020}{article}{}
      \name{author}{26}{}{%
        {{hash=db2b4761cc46be347b418e68660c9554}{%
           family={Harris},
           familyi={H\bibinitperiod},
           given={Charles\bibnamedelima R.},
           giveni={C\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=b053969d2c6a9ec8689980fb6463cd56}{%
           family={Millman},
           familyi={M\bibinitperiod},
           given={K.\bibnamedelimi Jarrod},
           giveni={K\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=9a5a65a789013a8d1e8035ec28df9b6e}{%
           family={Walt},
           familyi={W\bibinitperiod},
           given={Stéfan\bibnamedelima J.},
           giveni={S\bibinitperiod\bibinitdelim J\bibinitperiod},
           prefix={van\bibnamedelima der},
           prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=646fbfe08374cc41c2f9bd971d8c4725}{%
           family={Gommers},
           familyi={G\bibinitperiod},
           given={Ralf},
           giveni={R\bibinitperiod}}}%
        {{hash=18703a2bb6a62484483c193a212da2f8}{%
           family={Virtanen},
           familyi={V\bibinitperiod},
           given={Pauli},
           giveni={P\bibinitperiod}}}%
        {{hash=9fd9ed8466bbb96364ae008f2a665e6e}{%
           family={Cournapeau},
           familyi={C\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=4321feefe3d5715c6fe12639d2a1f7f1}{%
           family={Wieser},
           familyi={W\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=e8d1b0c2bff88b6bf54e2e934e2853f6}{%
           family={Taylor},
           familyi={T\bibinitperiod},
           given={Julian},
           giveni={J\bibinitperiod}}}%
        {{hash=265437a4424f0b845118651d7d04ab93}{%
           family={Berg},
           familyi={B\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=e4d23819637a961e0e598e1c82a8f10d}{%
           family={Smith},
           familyi={S\bibinitperiod},
           given={Nathaniel\bibnamedelima J.},
           giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=9ad1d38817acd2f00cb7f324ec7d37ea}{%
           family={Kern},
           familyi={K\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=8ae844f84614690134ff5d8252fdc430}{%
           family={Picus},
           familyi={P\bibinitperiod},
           given={Matti},
           giveni={M\bibinitperiod}}}%
        {{hash=4a22828b76006f42d747b699d2ef5167}{%
           family={Hoyer},
           familyi={H\bibinitperiod},
           given={Stephan},
           giveni={S\bibinitperiod}}}%
        {{hash=10aba85d08ce22d724b21878b93f590a}{%
           family={Kerkwijk},
           familyi={K\bibinitperiod},
           given={Marten\bibnamedelima H.},
           giveni={M\bibinitperiod\bibinitdelim H\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
        {{hash=626cc151613864abeb653c0d8172d98c}{%
           family={Brett},
           familyi={B\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=a989405517bbf67ae7b16be1d03c0081}{%
           family={Haldane},
           familyi={H\bibinitperiod},
           given={Allan},
           giveni={A\bibinitperiod}}}%
        {{hash=9517b38a3050d34c03ac479ee122f64a}{%
           family={Río},
           familyi={R\bibinitperiod},
           given={Jaime\bibnamedelima Fernández},
           giveni={J\bibinitperiod\bibinitdelim F\bibinitperiod},
           prefix={del},
           prefixi={d\bibinitperiod}}}%
        {{hash=76256cb9e4c31e0a4744b1b49a6199f4}{%
           family={Wiebe},
           familyi={W\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
        {{hash=3d6efaaa3d9682e20787eb06ff70a3d7}{%
           family={Peterson},
           familyi={P\bibinitperiod},
           given={Pearu},
           giveni={P\bibinitperiod}}}%
        {{hash=aac4486f60accb8b7db22ef300423e11}{%
           family={Gérard-Marchant},
           familyi={G\bibinithyphendelim M\bibinitperiod},
           given={Pierre},
           giveni={P\bibinitperiod}}}%
        {{hash=22fcbc93c38819e962968aad3e3cba7f}{%
           family={Sheppard},
           familyi={S\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=fbb0c40f5d70be8ce47ce9daafdf5749}{%
           family={Reddy},
           familyi={R\bibinitperiod},
           given={Tyler},
           giveni={T\bibinitperiod}}}%
        {{hash=4c7e4c94b846fa41e2fc0a88e0dc656d}{%
           family={Weckesser},
           familyi={W\bibinitperiod},
           given={Warren},
           giveni={W\bibinitperiod}}}%
        {{hash=c588b2e35e75bba896fc5677afb52fa9}{%
           family={Abbasi},
           familyi={A\bibinitperiod},
           given={Hameer},
           giveni={H\bibinitperiod}}}%
        {{hash=52e2684c111b194e1f15fa06b0ce4544}{%
           family={Gohlke},
           familyi={G\bibinitperiod},
           given={Christoph},
           giveni={C\bibinitperiod}}}%
        {{hash=d500f4849030f34359cdb3e1513acf83}{%
           family={Oliphant},
           familyi={O\bibinitperiod},
           given={Travis\bibnamedelima E.},
           giveni={T\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{754b3dec5e203c19b9a5798bbb902222}
      \strng{fullhash}{bba1ae456b2655ba67cf6cee2d142dfc}
      \strng{bibnamehash}{754b3dec5e203c19b9a5798bbb902222}
      \strng{authorbibnamehash}{754b3dec5e203c19b9a5798bbb902222}
      \strng{authornamehash}{754b3dec5e203c19b9a5798bbb902222}
      \strng{authorfullhash}{bba1ae456b2655ba67cf6cee2d142dfc}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.}
      \field{issn}{1476-4687}
      \field{journaltitle}{Nature}
      \field{month}{9}
      \field{note}{Number: 7825 Publisher: Nature Publishing Group}
      \field{number}{7825}
      \field{title}{Array programming with {NumPy}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{volume}{585}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{357\bibrangedash 362}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1038/s41586-020-2649-2
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/dmizrahi/Zotero/storage/P7YJS485/Harris et al. - 2020 - Array programming with NumPy.pdf:application/pdf;Snapshot:/Users/dmizrahi/Zotero/storage/QXPDB6IQ/s41586-020-2649-2.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41586-020-2649-2
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41586-020-2649-2
      \endverb
    \endentry
    \entry{virtanen_scipy_2020}{article}{}
      \name{author}{34}{}{%
        {{hash=18703a2bb6a62484483c193a212da2f8}{%
           family={Virtanen},
           familyi={V\bibinitperiod},
           given={Pauli},
           giveni={P\bibinitperiod}}}%
        {{hash=646fbfe08374cc41c2f9bd971d8c4725}{%
           family={Gommers},
           familyi={G\bibinitperiod},
           given={Ralf},
           giveni={R\bibinitperiod}}}%
        {{hash=d500f4849030f34359cdb3e1513acf83}{%
           family={Oliphant},
           familyi={O\bibinitperiod},
           given={Travis\bibnamedelima E.},
           giveni={T\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=35bb9c71f55048509a3e9018c349ed73}{%
           family={Haberland},
           familyi={H\bibinitperiod},
           given={Matt},
           giveni={M\bibinitperiod}}}%
        {{hash=fbb0c40f5d70be8ce47ce9daafdf5749}{%
           family={Reddy},
           familyi={R\bibinitperiod},
           given={Tyler},
           giveni={T\bibinitperiod}}}%
        {{hash=9fd9ed8466bbb96364ae008f2a665e6e}{%
           family={Cournapeau},
           familyi={C\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=09a667aa6a26526bfcccb2676a494e55}{%
           family={Burovski},
           familyi={B\bibinitperiod},
           given={Evgeni},
           giveni={E\bibinitperiod}}}%
        {{hash=3d6efaaa3d9682e20787eb06ff70a3d7}{%
           family={Peterson},
           familyi={P\bibinitperiod},
           given={Pearu},
           giveni={P\bibinitperiod}}}%
        {{hash=4c7e4c94b846fa41e2fc0a88e0dc656d}{%
           family={Weckesser},
           familyi={W\bibinitperiod},
           given={Warren},
           giveni={W\bibinitperiod}}}%
        {{hash=7447cb057596bc2645d3980bb04f5c78}{%
           family={Bright},
           familyi={B\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod}}}%
        {{hash=9a5a65a789013a8d1e8035ec28df9b6e}{%
           family={Walt},
           familyi={W\bibinitperiod},
           given={Stéfan\bibnamedelima J.},
           giveni={S\bibinitperiod\bibinitdelim J\bibinitperiod},
           prefix={van\bibnamedelima der},
           prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=626cc151613864abeb653c0d8172d98c}{%
           family={Brett},
           familyi={B\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=57849e8550281b202bd611bf6f11e14b}{%
           family={Wilson},
           familyi={W\bibinitperiod},
           given={Joshua},
           giveni={J\bibinitperiod}}}%
        {{hash=b053969d2c6a9ec8689980fb6463cd56}{%
           family={Millman},
           familyi={M\bibinitperiod},
           given={K.\bibnamedelimi Jarrod},
           giveni={K\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=fbaf80580622bd40577f4a6d38021c0a}{%
           family={Mayorov},
           familyi={M\bibinitperiod},
           given={Nikolay},
           giveni={N\bibinitperiod}}}%
        {{hash=7bcf847eaccba039f7a4523540673aea}{%
           family={Nelson},
           familyi={N\bibinitperiod},
           given={Andrew\bibnamedelimb R.\bibnamedelimi J.},
           giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=4b3d26f886661aa723985bcfd835ba18}{%
           family={Jones},
           familyi={J\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=9ad1d38817acd2f00cb7f324ec7d37ea}{%
           family={Kern},
           familyi={K\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=8d336f110675c46226ece1db501ce712}{%
           family={Larson},
           familyi={L\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=65b1934a87acb0abe09c469aaf11c326}{%
           family={Carey},
           familyi={C\bibinitperiod},
           given={C.\bibnamedelimi J.},
           giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=9989e8a18827e34b15112d671f52bd35}{%
           family={Polat},
           familyi={P\bibinitperiod},
           given={İlhan},
           giveni={İ\bibinitperiod}}}%
        {{hash=b8b88d61c79de60e6e1b5d44e03f5dec}{%
           family={Feng},
           familyi={F\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod}}}%
        {{hash=bf4be16325cb4f641345ca394443fd18}{%
           family={Moore},
           familyi={M\bibinitperiod},
           given={Eric\bibnamedelima W.},
           giveni={E\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=0fd9a0e34f1b2adda41357c948d14986}{%
           family={VanderPlas},
           familyi={V\bibinitperiod},
           given={Jake},
           giveni={J\bibinitperiod}}}%
        {{hash=c6a95a8ced3b86b4e7e60a74bc6ebf5a}{%
           family={Laxalde},
           familyi={L\bibinitperiod},
           given={Denis},
           giveni={D\bibinitperiod}}}%
        {{hash=85242652d69220e83cf71ceb8d90a8cb}{%
           family={Perktold},
           familyi={P\bibinitperiod},
           given={Josef},
           giveni={J\bibinitperiod}}}%
        {{hash=5bca159e697db439e23b8947dfa4b614}{%
           family={Cimrman},
           familyi={C\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=70b659f5067a8a2efbee66f770681598}{%
           family={Henriksen},
           familyi={H\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
        {{hash=fa5163c76600eb11a4d07a28f0701cb0}{%
           family={Quintero},
           familyi={Q\bibinitperiod},
           given={E.\bibnamedelimi A.},
           giveni={E\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=db2b4761cc46be347b418e68660c9554}{%
           family={Harris},
           familyi={H\bibinitperiod},
           given={Charles\bibnamedelima R.},
           giveni={C\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=7d86aea5ad1f2b4e27f2f014c71712c2}{%
           family={Archibald},
           familyi={A\bibinitperiod},
           given={Anne\bibnamedelima M.},
           giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=3876f7c3dbbb1a17823dcd135d07cfc6}{%
           family={Ribeiro},
           familyi={R\bibinitperiod},
           given={Antônio\bibnamedelima H.},
           giveni={A\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=bab4e5caee2d67831e464ce575022b37}{%
           family={Pedregosa},
           familyi={P\bibinitperiod},
           given={Fabian},
           giveni={F\bibinitperiod}}}%
        {{hash=1af2e1049c0f42049401999babb9f7b2}{%
           family={Mulbregt},
           familyi={M\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{f252038edd7f112d75da3bf0c1edecbc}
      \strng{fullhash}{f250109471b6de16324d417ff6e81e90}
      \strng{bibnamehash}{f252038edd7f112d75da3bf0c1edecbc}
      \strng{authorbibnamehash}{f252038edd7f112d75da3bf0c1edecbc}
      \strng{authornamehash}{f252038edd7f112d75da3bf0c1edecbc}
      \strng{authorfullhash}{f250109471b6de16324d417ff6e81e90}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.}
      \field{issn}{1548-7105}
      \field{journaltitle}{Nature Methods}
      \field{month}{3}
      \field{note}{Number: 3 Publisher: Nature Publishing Group}
      \field{number}{3}
      \field{shorttitle}{{SciPy} 1.0}
      \field{title}{{SciPy} 1.0: fundamental algorithms for scientific computing in {Python}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{volume}{17}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{261\bibrangedash 272}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1038/s41592-019-0686-2
      \endverb
      \verb{file}
      \verb Full Text PDF:/Users/dmizrahi/Zotero/storage/LDXSIR9T/Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf:application/pdf;Snapshot:/Users/dmizrahi/Zotero/storage/V2CIH5YS/s41592-019-0686-2.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41592-019-0686-2
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41592-019-0686-2
      \endverb
    \endentry
    \entry{yadan_hydra_2019}{book}{}
      \name{author}{1}{}{%
        {{hash=0c7d3557b383c6e62dcc2b94cc7612b5}{%
           family={Yadan},
           familyi={Y\bibinitperiod},
           given={Omry},
           giveni={O\bibinitperiod}}}%
      }
      \strng{namehash}{0c7d3557b383c6e62dcc2b94cc7612b5}
      \strng{fullhash}{0c7d3557b383c6e62dcc2b94cc7612b5}
      \strng{bibnamehash}{0c7d3557b383c6e62dcc2b94cc7612b5}
      \strng{authorbibnamehash}{0c7d3557b383c6e62dcc2b94cc7612b5}
      \strng{authornamehash}{0c7d3557b383c6e62dcc2b94cc7612b5}
      \strng{authorfullhash}{0c7d3557b383c6e62dcc2b94cc7612b5}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Hydra - {A} framework for elegantly configuring complex applications}
      \field{year}{2019}
      \verb{urlraw}
      \verb https://github.com/facebookresearch/hydra
      \endverb
      \verb{url}
      \verb https://github.com/facebookresearch/hydra
      \endverb
    \endentry
    \entry{paszke_pytorch_2019}{article}{}
      \name{author}{21}{}{%
        {{hash=56bf0b340039cf8594436a624ff548a9}{%
           family={Paszke},
           familyi={P\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=4ba5062e5919c814aceec188d54c01f2}{%
           family={Gross},
           familyi={G\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod}}}%
        {{hash=e5dfae4582081d649e3a0d5342050016}{%
           family={Massa},
           familyi={M\bibinitperiod},
           given={Francisco},
           giveni={F\bibinitperiod}}}%
        {{hash=b5815e1692fa2d0c1f44eecf509bd7c4}{%
           family={Lerer},
           familyi={L\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=b75383e6b48c8360c7a60031424c85cf}{%
           family={Bradbury},
           familyi={B\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=f897ed422c34d95af2e22778dfc2607e}{%
           family={Chanan},
           familyi={C\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=046269e070246feb6f394141db80ed87}{%
           family={Killeen},
           familyi={K\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
        {{hash=c40352c194e60a3ef458ee7e8685afb5}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Zeming},
           giveni={Z\bibinitperiod}}}%
        {{hash=6e45f49ec618e619efad90c8e8a61f0c}{%
           family={Gimelshein},
           familyi={G\bibinitperiod},
           given={Natalia},
           giveni={N\bibinitperiod}}}%
        {{hash=f65a80959d520337ae99a0798515036c}{%
           family={Antiga},
           familyi={A\bibinitperiod},
           given={Luca},
           giveni={L\bibinitperiod}}}%
        {{hash=954cf7680b6ce14813973eccdca3c4bc}{%
           family={Desmaison},
           familyi={D\bibinitperiod},
           given={Alban},
           giveni={A\bibinitperiod}}}%
        {{hash=048232cf7c525fbc0bc93052fe8cee03}{%
           family={Köpf},
           familyi={K\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod}}}%
        {{hash=b9e701339e56fd0b171145b08288a1b7}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=42ac264897098b400e1367e5922c9b0d}{%
           family={DeVito},
           familyi={D\bibinitperiod},
           given={Zach},
           giveni={Z\bibinitperiod}}}%
        {{hash=d814afaa50b9e22ab92cc9f8f9a9e43a}{%
           family={Raison},
           familyi={R\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=3feeeebee8583ecc208f7fb3e0a55068}{%
           family={Tejani},
           familyi={T\bibinitperiod},
           given={Alykhan},
           giveni={A\bibinitperiod}}}%
        {{hash=e18536d5cb7543731fbf2ca1a4908732}{%
           family={Chilamkurthy},
           familyi={C\bibinitperiod},
           given={Sasank},
           giveni={S\bibinitperiod}}}%
        {{hash=0a0b028c6b85c46f368317d0c5bfe3a0}{%
           family={Steiner},
           familyi={S\bibinitperiod},
           given={Benoit},
           giveni={B\bibinitperiod}}}%
        {{hash=998a001f16bb57c079c1d5afb1cb02c8}{%
           family={Fang},
           familyi={F\bibinitperiod},
           given={Lu},
           giveni={L\bibinitperiod}}}%
        {{hash=3f19c633bbfb847db6a0e71d3659eacd}{%
           family={Bai},
           familyi={B\bibinitperiod},
           given={Junjie},
           giveni={J\bibinitperiod}}}%
        {{hash=8ef51a0906e47d2b4472c4e714ed598f}{%
           family={Chintala},
           familyi={C\bibinitperiod},
           given={Soumith},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{fullhash}{4842db6c92a33147f588935fdde44a69}
      \strng{bibnamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authorbibnamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authornamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authorfullhash}{4842db6c92a33147f588935fdde44a69}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.}
      \field{annotation}{Comment: 12 pages, 3 figures, NeurIPS 2019}
      \field{journaltitle}{arXiv:1912.01703 [cs, stat]}
      \field{month}{12}
      \field{note}{arXiv: 1912.01703}
      \field{shorttitle}{{PyTorch}}
      \field{title}{{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/dmizrahi/Zotero/storage/KSCSIB2A/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/dmizrahi/Zotero/storage/2VH9Q8F7/1912.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1912.01703
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1912.01703
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Machine Learning}
    \endentry
    \entry{lecun_gradient-based_1998}{article}{}
      \name{author}{4}{}{%
        {{hash=c7a5b066431b979612b716532b228554}{%
           family={Lecun},
           familyi={L\bibinitperiod},
           given={Y.},
           giveni={Y\bibinitperiod}}}%
        {{hash=bbfb0f3936c83b7b099561e6f0e32ef3}{%
           family={Bottou},
           familyi={B\bibinitperiod},
           given={L.},
           giveni={L\bibinitperiod}}}%
        {{hash=419350ebbeb4eba5351469f378dee007}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Y.},
           giveni={Y\bibinitperiod}}}%
        {{hash=00f962380d25c4d7f23fa6c7e926c3ed}{%
           family={Haffner},
           familyi={H\bibinitperiod},
           given={P.},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{dd2ddc978fe083bcff1aa1379cd19643}
      \strng{fullhash}{4dd3ca3cdc8023700c28169734d6ad61}
      \strng{bibnamehash}{4dd3ca3cdc8023700c28169734d6ad61}
      \strng{authorbibnamehash}{4dd3ca3cdc8023700c28169734d6ad61}
      \strng{authornamehash}{dd2ddc978fe083bcff1aa1379cd19643}
      \strng{authorfullhash}{4dd3ca3cdc8023700c28169734d6ad61}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.}
      \field{issn}{1558-2256}
      \field{journaltitle}{Proceedings of the IEEE}
      \field{month}{11}
      \field{note}{Conference Name: Proceedings of the IEEE}
      \field{number}{11}
      \field{title}{Gradient-based learning applied to document recognition}
      \field{volume}{86}
      \field{year}{1998}
      \field{pages}{2278\bibrangedash 2324}
      \range{pages}{47}
      \verb{doi}
      \verb 10.1109/5.726791
      \endverb
      \verb{file}
      \verb Submitted Version:/Users/dmizrahi/Zotero/storage/BBG4DINU/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/dmizrahi/Zotero/storage/B9ZRC2FI/726791.html:text/html
      \endverb
      \keyw{2D shape variability,back-propagation,backpropagation,Character recognition,cheque reading,complex decision surface synthesis,convolution,convolutional neural network character recognizers,document recognition,document recognition systems,Feature extraction,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,GTN,handwritten character recognition,handwritten digit recognition task,Hidden Markov models,high-dimensional patterns,language modeling,Machine learning,Multi-layer neural network,multilayer neural networks,multilayer perceptrons,multimodule systems,Neural networks,optical character recognition,Optical character recognition software,Optical computing,Pattern recognition,performance measure minimization,Principal component analysis,segmentation recognition}
    \endentry
    \entry{kingma_adam_2017}{article}{}
      \name{author}{2}{}{%
        {{hash=b6fbd171848aad4edf3925543f1f1522}{%
           family={Kingma},
           familyi={K\bibinitperiod},
           given={Diederik\bibnamedelima P.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=8aa66e8231cc2fdbe67aa4f18ca970c6}{%
           family={Ba},
           familyi={B\bibinitperiod},
           given={Jimmy},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{fullhash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{bibnamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authorbibnamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authornamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authorfullhash}{a09df9f123146b8e2c7f1134c9496932}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}
      \field{annotation}{Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015}
      \field{journaltitle}{arXiv:1412.6980 [cs]}
      \field{month}{1}
      \field{note}{arXiv: 1412.6980}
      \field{shorttitle}{Adam}
      \field{title}{Adam: {A} {Method} for {Stochastic} {Optimization}}
      \field{urlday}{16}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/dmizrahi/Zotero/storage/BWURXMPL/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/dmizrahi/Zotero/storage/JNUEC92Z/1412.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1412.6980
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1412.6980
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{krizhevsky_learning_2009}{article}{}
      \name{author}{2}{}{%
        {{hash=c5e3a676e2ac1164b3afcd539c131fc9}{%
           family={Krizhevsky},
           familyi={K\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{db1738a0374d10d152a785eb2a94ce8c}
      \strng{fullhash}{db1738a0374d10d152a785eb2a94ce8c}
      \strng{bibnamehash}{db1738a0374d10d152a785eb2a94ce8c}
      \strng{authorbibnamehash}{db1738a0374d10d152a785eb2a94ce8c}
      \strng{authornamehash}{db1738a0374d10d152a785eb2a94ce8c}
      \strng{authorfullhash}{db1738a0374d10d152a785eb2a94ce8c}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}}
      \field{year}{2009}
      \field{pages}{60}
      \range{pages}{1}
      \verb{file}
      \verb Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf:/Users/dmizrahi/Zotero/storage/YWPIMQDQ/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf:application/pdf
      \endverb
    \endentry
    \entry{zagoruyko_wide_2016}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=23bcbde34947ed4fd04cc55ff956c83e}{%
           family={Zagoruyko},
           familyi={Z\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
        {{hash=1b19df6325edc9e76749ac30a9d962c7}{%
           family={Komodakis},
           familyi={K\bibinitperiod},
           given={Nikos},
           giveni={N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{location}{1}{%
        {York, UK}%
      }
      \list{publisher}{1}{%
        {British Machine Vision Association}%
      }
      \strng{namehash}{d6318237b805b7959eba1465dd40cf0a}
      \strng{fullhash}{d6318237b805b7959eba1465dd40cf0a}
      \strng{bibnamehash}{d6318237b805b7959eba1465dd40cf0a}
      \strng{authorbibnamehash}{d6318237b805b7959eba1465dd40cf0a}
      \strng{authornamehash}{d6318237b805b7959eba1465dd40cf0a}
      \strng{authorfullhash}{d6318237b805b7959eba1465dd40cf0a}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Procedings of the {British} {Machine} {Vision} {Conference} 2016}
      \field{isbn}{978-1-901725-59-9}
      \field{title}{Wide {Residual} {Networks}}
      \field{urlday}{16}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2016}
      \field{urldateera}{ce}
      \field{pages}{87.1\bibrangedash 87.12}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.5244/C.30.87
      \endverb
      \verb{urlraw}
      \verb http://www.bmva.org/bmvc/2016/papers/paper087/index.html
      \endverb
      \verb{url}
      \verb http://www.bmva.org/bmvc/2016/papers/paper087/index.html
      \endverb
    \endentry
    \entry{he_deep_2015}{article}{}
      \name{author}{4}{}{%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=5e72bc22dbcf0984c6d113d280e36990}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Xiangyu},
           giveni={X\bibinitperiod}}}%
        {{hash=bb295293acacd54387339079ebbe4ead}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Shaoqing},
           giveni={S\bibinitperiod}}}%
        {{hash=f85751488058842b5777c7b4074077b5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{fullhash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \strng{bibnamehash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \strng{authorbibnamehash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \strng{authornamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authorfullhash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}
      \field{annotation}{Comment: Tech report}
      \field{journaltitle}{arXiv:1512.03385 [cs]}
      \field{month}{12}
      \field{note}{arXiv: 1512.03385}
      \field{title}{Deep {Residual} {Learning} for {Image} {Recognition}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2015}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/dmizrahi/Zotero/storage/9SKE8IKH/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/dmizrahi/Zotero/storage/AG3GDRKJ/1512.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1512.03385
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1512.03385
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \endentry
    \entry{liu_kuangliupytorch-cifar_2017}{misc}{}
      \name{author}{1}{}{%
        {{hash=93982da99b1629ff5b6833c91a8a11f5}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Kuang},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{93982da99b1629ff5b6833c91a8a11f5}
      \strng{fullhash}{93982da99b1629ff5b6833c91a8a11f5}
      \strng{bibnamehash}{93982da99b1629ff5b6833c91a8a11f5}
      \strng{authorbibnamehash}{93982da99b1629ff5b6833c91a8a11f5}
      \strng{authornamehash}{93982da99b1629ff5b6833c91a8a11f5}
      \strng{authorfullhash}{93982da99b1629ff5b6833c91a8a11f5}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{95.47\% on CIFAR10 with PyTorch. Contribute to kuangliu/pytorch-cifar development by creating an account on GitHub.}
      \field{month}{12}
      \field{note}{original-date: 2017-01-21T05:43:20Z}
      \field{title}{kuangliu/pytorch-cifar}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://github.com/kuangliu/pytorch-cifar
      \endverb
      \verb{url}
      \verb https://github.com/kuangliu/pytorch-cifar
      \endverb
      \keyw{pytorch}
    \endentry
    \entry{imagenet_cvpr09}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=cbaf4f8829c5ef01cf9659d1d6975526}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
        {{hash=955e4fa0c47a616eecedbeeb06aa65fa}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={W.},
           giveni={W\bibinitperiod}}}%
        {{hash=edd3c4a3b6388614c15fdfa581b52069}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={R.},
           giveni={R\bibinitperiod}}}%
        {{hash=4db47e780a7092458880617d8c3231f7}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={L.-J.},
           giveni={L\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=1a48ecf9984adc4526895c8abf1d2b80}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={K.},
           giveni={K\bibinitperiod}}}%
        {{hash=f536a9246365f63717886015403a0964}{%
           family={Fei-Fei},
           familyi={F\bibinithyphendelim F\bibinitperiod},
           given={L.},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{e727e8b64d4e11a52e90122d423e12b6}
      \strng{fullhash}{fec3a590a384f905f4a8183a7c8ec390}
      \strng{bibnamehash}{fec3a590a384f905f4a8183a7c8ec390}
      \strng{authorbibnamehash}{fec3a590a384f905f4a8183a7c8ec390}
      \strng{authornamehash}{e727e8b64d4e11a52e90122d423e12b6}
      \strng{authorfullhash}{fec3a590a384f905f4a8183a7c8ec390}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{CVPR09}
      \field{title}{{ImageNet: A Large-Scale Hierarchical Image Database}}
      \field{year}{2009}
    \endentry
    \entry{devries_improved_2017}{article}{}
      \name{author}{2}{}{%
        {{hash=5f72faa67f3e3eeb5ab473ef067062aa}{%
           family={DeVries},
           familyi={D\bibinitperiod},
           given={Terrance},
           giveni={T\bibinitperiod}}}%
        {{hash=8c57b78bacc3da92f8aa3623167ea5de}{%
           family={Taylor},
           familyi={T\bibinitperiod},
           given={Graham\bibnamedelima W.},
           giveni={G\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \strng{namehash}{c654df171568208867bf898ab0b57ea8}
      \strng{fullhash}{c654df171568208867bf898ab0b57ea8}
      \strng{bibnamehash}{c654df171568208867bf898ab0b57ea8}
      \strng{authorbibnamehash}{c654df171568208867bf898ab0b57ea8}
      \strng{authornamehash}{c654df171568208867bf898ab0b57ea8}
      \strng{authorfullhash}{c654df171568208867bf898ab0b57ea8}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout}
      \field{journaltitle}{arXiv:1708.04552 [cs]}
      \field{month}{11}
      \field{note}{arXiv: 1708.04552}
      \field{title}{Improved {Regularization} of {Convolutional} {Neural} {Networks} with {Cutout}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/dmizrahi/Zotero/storage/WZ36PFZ9/DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/dmizrahi/Zotero/storage/95ND9PEY/1708.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1708.04552
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1708.04552
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \endentry
    \entry{zhang_mixup_2018}{article}{}
      \name{author}{4}{}{%
        {{hash=2055fe99efa178d6f877d718047b0390}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Hongyi},
           giveni={H\bibinitperiod}}}%
        {{hash=c55edd870afd5174d08d32e8560235f0}{%
           family={Cisse},
           familyi={C\bibinitperiod},
           given={Moustapha},
           giveni={M\bibinitperiod}}}%
        {{hash=ea1ca71b064fbb7ec15bd2e49e287ea9}{%
           family={Dauphin},
           familyi={D\bibinitperiod},
           given={Yann\bibnamedelima N.},
           giveni={Y\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=47d681c0cbae75196012a538c46ccc06}{%
           family={Lopez-Paz},
           familyi={L\bibinithyphendelim P\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{47ed15e39b226ffdc3da65f48de59b1f}
      \strng{fullhash}{99e17545100e06ee9db2c0f295e2db48}
      \strng{bibnamehash}{99e17545100e06ee9db2c0f295e2db48}
      \strng{authorbibnamehash}{99e17545100e06ee9db2c0f295e2db48}
      \strng{authornamehash}{47ed15e39b226ffdc3da65f48de59b1f}
      \strng{authorfullhash}{99e17545100e06ee9db2c0f295e2db48}
      \field{extraname}{2}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.}
      \field{annotation}{Comment: ICLR camera ready version. Changes vs V1: fix repo URL; add ablation studies; add mixup + dropout etc}
      \field{journaltitle}{arXiv:1710.09412 [cs, stat]}
      \field{month}{4}
      \field{note}{arXiv: 1710.09412}
      \field{shorttitle}{mixup}
      \field{title}{mixup: {Beyond} {Empirical} {Risk} {Minimization}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2018}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/dmizrahi/Zotero/storage/9X9SJP4N/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:application/pdf;arXiv.org Snapshot:/Users/dmizrahi/Zotero/storage/VI6QZZDF/1710.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1710.09412
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1710.09412
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{li_dividemix_2020}{article}{}
      \name{author}{3}{}{%
        {{hash=1c6e653d1c884531f9bc28d48227c534}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Junnan},
           giveni={J\bibinitperiod}}}%
        {{hash=d5670b2600fea169724521e252d9d09d}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=a3873cf857c07c3080a03f272aea133d}{%
           family={Hoi},
           familyi={H\bibinitperiod},
           given={Steven\bibnamedelimb C.\bibnamedelimi H.},
           giveni={S\bibinitperiod\bibinitdelim C\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \strng{namehash}{b7052cffcd7c78558dfb67fc9f25b370}
      \strng{fullhash}{b7052cffcd7c78558dfb67fc9f25b370}
      \strng{bibnamehash}{b7052cffcd7c78558dfb67fc9f25b370}
      \strng{authorbibnamehash}{b7052cffcd7c78558dfb67fc9f25b370}
      \strng{authornamehash}{b7052cffcd7c78558dfb67fc9f25b370}
      \strng{authorfullhash}{b7052cffcd7c78558dfb67fc9f25b370}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .}
      \field{journaltitle}{arXiv:2002.07394 [cs]}
      \field{month}{2}
      \field{note}{arXiv: 2002.07394}
      \field{shorttitle}{{DivideMix}}
      \field{title}{{DivideMix}: {Learning} with {Noisy} {Labels} as {Semi}-supervised {Learning}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/dmizrahi/Zotero/storage/86P5MZFV/Li et al. - 2020 - DivideMix Learning with Noisy Labels as Semi-supe.pdf:application/pdf;arXiv.org Snapshot:/Users/dmizrahi/Zotero/storage/QGG6D89C/2002.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2002.07394
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2002.07394
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \endentry
    \entry{zhang_lookahead_2019}{article}{}
      \name{author}{4}{}{%
        {{hash=c669f7c134cbc39b97865180aedadd29}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Michael\bibnamedelima R.},
           giveni={M\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=a08d31cc0f2c7caa38fec4cc2a165e7f}{%
           family={Lucas},
           familyi={L\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
        {{hash=8aa66e8231cc2fdbe67aa4f18ca970c6}{%
           family={Ba},
           familyi={B\bibinitperiod},
           given={Jimmy},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{e80afa96687f43490e2e9a979515fde9}
      \strng{fullhash}{55f8ffb6c0c2feaacd2594d0d79b1b0f}
      \strng{bibnamehash}{55f8ffb6c0c2feaacd2594d0d79b1b0f}
      \strng{authorbibnamehash}{55f8ffb6c0c2feaacd2594d0d79b1b0f}
      \strng{authornamehash}{e80afa96687f43490e2e9a979515fde9}
      \strng{authorfullhash}{55f8ffb6c0c2feaacd2594d0d79b1b0f}
      \field{extraname}{3}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.}
      \field{annotation}{Comment: Accepted to Neural Information Processing Systems 2019. Code available at: https://github.com/michaelrzhang/lookahead}
      \field{journaltitle}{arXiv:1907.08610 [cs, stat]}
      \field{month}{12}
      \field{note}{arXiv: 1907.08610}
      \field{shorttitle}{Lookahead {Optimizer}}
      \field{title}{Lookahead {Optimizer}: k steps forward, 1 step back}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/dmizrahi/Zotero/storage/M7N3FNVB/Zhang et al. - 2019 - Lookahead Optimizer k steps forward, 1 step back.pdf:application/pdf;arXiv.org Snapshot:/Users/dmizrahi/Zotero/storage/R7ETSXUP/1907.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1907.08610
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1907.08610
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
    \endentry
    \entry{nesterov_method_1983}{article}{}
      \name{author}{1}{}{%
        {{hash=0459b44bb53eeaf227660839b9d8feeb}{%
           family={Nesterov},
           familyi={N\bibinitperiod},
           given={Y.\bibnamedelimi E.},
           giveni={Y\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \strng{namehash}{0459b44bb53eeaf227660839b9d8feeb}
      \strng{fullhash}{0459b44bb53eeaf227660839b9d8feeb}
      \strng{bibnamehash}{0459b44bb53eeaf227660839b9d8feeb}
      \strng{authorbibnamehash}{0459b44bb53eeaf227660839b9d8feeb}
      \strng{authornamehash}{0459b44bb53eeaf227660839b9d8feeb}
      \strng{authorfullhash}{0459b44bb53eeaf227660839b9d8feeb}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Dokl. Akad. Nauk SSSR}
      \field{title}{A method for solving the convex programming problem with convergence rate {O}(1/k{\textasciicircum}2)}
      \field{urlday}{24}
      \field{urlmonth}{10}
      \field{urlyear}{2020}
      \field{volume}{269}
      \field{year}{1983}
      \field{urldateera}{ce}
      \field{pages}{543\bibrangedash 547}
      \range{pages}{5}
      \verb{file}
      \verb A method for solving the convex programming problem with convergence rate O(1/k^2) Snapshot:/Users/dmizrahi/Zotero/storage/7DR6DEB8/10029946121.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://ci.nii.ac.jp/naid/10029946121/
      \endverb
      \verb{url}
      \verb https://ci.nii.ac.jp/naid/10029946121/
      \endverb
    \endentry
    \entry{sutskever_importance_2013}{article}{}
      \name{author}{4}{}{%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=6811d5cc72244219e1b98cf2bb1b64f1}{%
           family={Martens},
           familyi={M\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=ba040554cec389bc8b5dc4b20d44218c}{%
           family={Dahl},
           familyi={D\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{07c96466e3b9e93bc5fe1db0e089e9d6}
      \strng{fullhash}{9cdbdb5ed37d642e2881c7a467f38028}
      \strng{bibnamehash}{9cdbdb5ed37d642e2881c7a467f38028}
      \strng{authorbibnamehash}{9cdbdb5ed37d642e2881c7a467f38028}
      \strng{authornamehash}{07c96466e3b9e93bc5fe1db0e089e9d6}
      \strng{authorfullhash}{9cdbdb5ed37d642e2881c7a467f38028}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We ﬁnd that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.}
      \field{title}{On the importance of initialization and momentum in deep learning}
      \field{year}{2013}
      \field{pages}{14}
      \range{pages}{1}
      \verb{file}
      \verb Sutskever et al. - On the importance of initialization and momentum i.pdf:/Users/dmizrahi/Zotero/storage/7CXXTVLF/momentum.pdf:application/pdf
      \endverb
    \endentry
    \entry{micikevicius_mixed_2017}{article}{}
      \name{author}{11}{}{%
        {{hash=d7c3b32c10d0f73ef681b6074aa1cdd4}{%
           family={Micikevicius},
           familyi={M\bibinitperiod},
           given={Paulius},
           giveni={P\bibinitperiod}}}%
        {{hash=a2da348cbbad750c6fe76dda9c35ffcd}{%
           family={Narang},
           familyi={N\bibinitperiod},
           given={Sharan},
           giveni={S\bibinitperiod}}}%
        {{hash=2775ac930f1eb8512a1187294d6fcb98}{%
           family={Alben},
           familyi={A\bibinitperiod},
           given={Jonah},
           giveni={J\bibinitperiod}}}%
        {{hash=fe0b5c62f5fbbcbe8b3b69115795d3e5}{%
           family={Diamos},
           familyi={D\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=6aa92a937d4d30dd0b5ec0eecbad1bf1}{%
           family={Elsen},
           familyi={E\bibinitperiod},
           given={Erich},
           giveni={E\bibinitperiod}}}%
        {{hash=390131310e5498d8c5fa5414afdc5f26}{%
           family={Garcia},
           familyi={G\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=57b6ee8703cf89267a2d51429a4e61a7}{%
           family={Ginsburg},
           familyi={G\bibinitperiod},
           given={Boris},
           giveni={B\bibinitperiod}}}%
        {{hash=e9b08b237b71fda19c9d090f30b37e4b}{%
           family={Houston},
           familyi={H\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=bf07d85e23f5d109ed19745b901d2ce8}{%
           family={Kuchaiev},
           familyi={K\bibinitperiod},
           given={Oleksii},
           giveni={O\bibinitperiod}}}%
        {{hash=d3fa1f906f9b4c19fe8931015c8908da}{%
           family={Venkatesh},
           familyi={V\bibinitperiod},
           given={Ganesh},
           giveni={G\bibinitperiod}}}%
        {{hash=d0d5a12b1f5bae45ea7780158b3b35c8}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Hao},
           giveni={H\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{edba223ee55e56879fca40d7bf77b7dc}
      \strng{fullhash}{fc75d779136c36bfa2f39068ce45bbba}
      \strng{bibnamehash}{edba223ee55e56879fca40d7bf77b7dc}
      \strng{authorbibnamehash}{edba223ee55e56879fca40d7bf77b7dc}
      \strng{authornamehash}{edba223ee55e56879fca40d7bf77b7dc}
      \strng{authorfullhash}{fc75d779136c36bfa2f39068ce45bbba}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.}
      \field{month}{10}
      \field{title}{Mixed {Precision} {Training}}
      \field{urlday}{16}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb Snapshot:/Users/dmizrahi/Zotero/storage/EL87SUW7/1710.html:text/html;Full Text PDF:/Users/dmizrahi/Zotero/storage/R6SBIXEU/Micikevicius et al. - 2017 - Mixed Precision Training.pdf:application/pdf;Snapshot:/Users/dmizrahi/Zotero/storage/TKXALYP3/1710.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://arxiv.org/abs/1710.03740v3
      \endverb
      \verb{url}
      \verb https://arxiv.org/abs/1710.03740v3
      \endverb
    \endentry
    \entry{akiba2019optuna}{misc}{}
      \name{author}{5}{}{%
        {{hash=2939d0d5b491018f7cc7812609071a79}{%
           family={Akiba},
           familyi={A\bibinitperiod},
           given={Takuya},
           giveni={T\bibinitperiod}}}%
        {{hash=92311081bb41fd01eb1ce06f61805909}{%
           family={Sano},
           familyi={S\bibinitperiod},
           given={Shotaro},
           giveni={S\bibinitperiod}}}%
        {{hash=a841fb8471f00b147fdd8954fb01c03f}{%
           family={Yanase},
           familyi={Y\bibinitperiod},
           given={Toshihiko},
           giveni={T\bibinitperiod}}}%
        {{hash=db73c7d77457602e9a75d75bbe181aba}{%
           family={Ohta},
           familyi={O\bibinitperiod},
           given={Takeru},
           giveni={T\bibinitperiod}}}%
        {{hash=d18b784f7f35111bc05bcbd245a34ce7}{%
           family={Koyama},
           familyi={K\bibinitperiod},
           given={Masanori},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{0da4c5b8107cb83a6bd4ba0c9b294621}
      \strng{fullhash}{6a8f61fd5680dbd348b2b27b4a54d0d8}
      \strng{bibnamehash}{6a8f61fd5680dbd348b2b27b4a54d0d8}
      \strng{authorbibnamehash}{6a8f61fd5680dbd348b2b27b4a54d0d8}
      \strng{authornamehash}{0da4c5b8107cb83a6bd4ba0c9b294621}
      \strng{authorfullhash}{6a8f61fd5680dbd348b2b27b4a54d0d8}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{title}{Optuna: A Next-generation Hyperparameter Optimization Framework}
      \field{year}{2019}
      \verb{eprint}
      \verb 1907.10902
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

