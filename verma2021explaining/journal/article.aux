\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@refcontext{none/global//global/global}
\abx@aux@cite{plumb2020explaining}
\abx@aux@segm{0}{0}{plumb2020explaining}
\abx@aux@cite{paszke2019pytorch}
\abx@aux@segm{0}{0}{paszke2019pytorch}
\HyPL@Entry{0<</S/D>>}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{american}{}
\pgfsyspdfmark {pgfid1}{8391059}{49131591}
\newmarginnote{note.1.1}{{1}{8391059sp}}
\abx@aux@cite{bishop:2006:PRML}
\abx@aux@segm{0}{0}{bishop:2006:PRML}
\abx@aux@cite{xie2018survey}
\abx@aux@segm{0}{0}{xie2018survey}
\abx@aux@segm{0}{0}{plumb2020explaining}
\abx@aux@cite{belinkov-etal-2020-interpretability}
\abx@aux@segm{0}{0}{belinkov-etal-2020-interpretability}
\abx@aux@cite{verma2020counterfactual}
\abx@aux@segm{0}{0}{verma2020counterfactual}
\abx@aux@segm{0}{0}{plumb2020explaining}
\abx@aux@cite{carvalho2019machine}
\abx@aux@segm{0}{0}{carvalho2019machine}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{3}{section.2}}
\newlabel{sec:claims}{{2}{3}{Methodology}{section.2}{}}
\newlabel{loss_function}{{1}{3}{Methodology}{equation.2.1}{}}
\abx@aux@segm{0}{0}{plumb2020explaining}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Metrics to evaluate Global Counterfactual Explanations}{4}{subsection.2.1}}
\newlabel{sec:metrics}{{2.1}{4}{Metrics to evaluate Global Counterfactual Explanations}{subsection.2.1}{}}
\abx@aux@segm{0}{0}{paszke2019pytorch}
\abx@aux@cite{van2008visualizing}
\abx@aux@segm{0}{0}{van2008visualizing}
\abx@aux@cite{scvis}
\abx@aux@segm{0}{0}{scvis}
\abx@aux@cite{bipolar}
\abx@aux@segm{0}{0}{bipolar}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Scope of Reproducibility}{5}{section.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology of Reproducibility}{5}{section.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model description}{5}{subsection.4.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Dataset Description}{5}{subsection.4.2}}
\abx@aux@cite{abadi2016tensorflow}
\abx@aux@segm{0}{0}{abadi2016tensorflow}
\abx@aux@cite{kingma2014autoencoding}
\abx@aux@segm{0}{0}{kingma2014autoencoding}
\abx@aux@segm{0}{0}{plumb2020explaining}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Hyperparameters}{6}{subsection.4.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Tensorflow \cite {abadi2016tensorflow} Experiments}{6}{paragraph*.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Pytorch}{6}{paragraph*.2}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Truncation Values (TV) and $\epsilon $ value used for each of the dataset.\relax }}{6}{table.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:tv}{{1}{6}{Truncation Values (TV) and $\epsilon $ value used for each of the dataset.\relax }{table.caption.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Experimental setup and code}{6}{subsection.4.4}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Explanations for the synthetic dataset as given by our implementation. Note that both DBM and TGT are able to infer that the $x_3$ is not causing any cluster. However, the authors' claim that TGT also discovers that $x_4$ doesn't cause any cluster cannot be verified.\relax }}{7}{table.caption.4}}
\newlabel{tab:synth}{{2}{7}{Explanations for the synthetic dataset as given by our implementation. Note that both DBM and TGT are able to infer that the $x_3$ is not causing any cluster. However, the authors' claim that TGT also discovers that $x_4$ doesn't cause any cluster cannot be verified.\relax }{table.caption.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{7}{section.5}}
\newlabel{sec:results}{{5}{7}{Results}{section.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Results reproducing original paper}{7}{subsection.5.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Coverage, Correctness and similarity}{7}{subsubsection.5.1.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of the metrics(Correctness, Coverage, and Similarity) across different datasets for reproduction experiments.\relax }}{7}{figure.caption.5}}
\newlabel{fig:reproduced_tgt_metrics}{{1}{7}{Comparison of the metrics(Correctness, Coverage, and Similarity) across different datasets for reproduction experiments.\relax }{figure.caption.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Explaining Causal Structure in the Synthetic Data}{7}{subsection.5.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Feature modifications}{8}{subsubsection.5.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Results beyond original paper}{8}{subsection.5.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}PyTorch replication}{8}{subsubsection.5.3.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}UCI Heart Disease and UCI Boston Housing Dataset}{8}{subsubsection.5.3.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results for the PyTorch replication for UCI Boston Housing and UCI Heart Disease dataset.\relax }}{8}{figure.caption.6}}
\newlabel{fig:pt_heart_housing}{{2}{8}{Results for the PyTorch replication for UCI Boston Housing and UCI Heart Disease dataset.\relax }{figure.caption.6}{}}
\abx@aux@segm{0}{0}{belinkov-etal-2020-interpretability}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Breast Cancer Wisconsin (Diagnostic) and Pima Indians Diabetes Database}{9}{subsubsection.5.3.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results of the PyTorch replication on PIMA Indians Diabetes and Breast Cancer Wisconsin (Diagnostic) Dataset.\relax }}{9}{figure.caption.7}}
\newlabel{fig:pt_diabetes_breast}{{3}{9}{Results of the PyTorch replication on PIMA Indians Diabetes and Breast Cancer Wisconsin (Diagnostic) Dataset.\relax }{figure.caption.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Scaling extension}{9}{subsubsection.5.3.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.5}Experiment with Modified Synthetic Data}{9}{subsubsection.5.3.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.6}Experiments with Probing Classifier}{9}{subsubsection.5.3.6}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The deltas($\delta _s$) and gammas($\gamma _s$) for the mapping from group 0 to group 1 on the modified synthetic dataset for regular TGT and TGT with scaling. \textit  {Cr} and \textit  {Cv} indicate correctness and coverage, respectively.\relax }}{10}{table.caption.8}}
\newlabel{tab:synth_modified_results}{{3}{10}{The deltas($\delta _s$) and gammas($\gamma _s$) for the mapping from group 0 to group 1 on the modified synthetic dataset for regular TGT and TGT with scaling. \textit {Cr} and \textit {Cv} indicate correctness and coverage, respectively.\relax }{table.caption.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Classification accuracy of probing classifier at different sparsity levels for Housing (left) and Iris (right) dataset.\relax }}{10}{figure.caption.9}}
\newlabel{fig:classification}{{4}{10}{Classification accuracy of probing classifier at different sparsity levels for Housing (left) and Iris (right) dataset.\relax }{figure.caption.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{10}{section.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7}Extra figures}{11}{section.7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Experiments on Corrupted datasets}{11}{subsection.7.1}}
\newlabel{app:t2c}{{7.1}{11}{Experiments on Corrupted datasets}{subsection.7.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Explanation for corrupted features on UCI Iris dataset. Feature modified is 1(Sepal Width). Left: Visualization of the TGT explanations on the modified dataset. Right: Visualization of the TGT explanations with scVIS retrained on the modified dataset. We observe that the TGT explanations are robust to the modifications.\relax }}{11}{figure.caption.10}}
\newlabel{fig:t2c-Iris}{{5}{11}{Explanation for corrupted features on UCI Iris dataset. Feature modified is 1(Sepal Width). Left: Visualization of the TGT explanations on the modified dataset. Right: Visualization of the TGT explanations with scVIS retrained on the modified dataset. We observe that the TGT explanations are robust to the modifications.\relax }{figure.caption.10}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Explanation for corrupted features on UCI Boston Housing dataset. We modify the features 1(ZN) and 9(TAX). Left: Visualization of the TGT explanations on the modified dataset. We observe that TGT returns noisy explanations in this case. Right: Visualization of the TGT explanations with scVIS retrained on the modified dataset. With retrained scVIS model, TGT is able to recover the modifications.\relax }}{12}{figure.caption.11}}
\newlabel{fig:t2c-Housing}{{6}{12}{Explanation for corrupted features on UCI Boston Housing dataset. We modify the features 1(ZN) and 9(TAX). Left: Visualization of the TGT explanations on the modified dataset. We observe that TGT returns noisy explanations in this case. Right: Visualization of the TGT explanations with scVIS retrained on the modified dataset. With retrained scVIS model, TGT is able to recover the modifications.\relax }{figure.caption.11}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Explanation for corrupted features on UCI Heart Disease dataset. Left: Visualization of the TGT explanations on the modified dataset. We modified the 6(restecg) and 8(exang). However, the TGT recovers modifications in features 2(cp), 5(fbs), and 10(slope) instead. Right: Visualization of the TGT explanations with scVIS retrained on the modified dataset. With retrained scVIS model, TGT recovers the modified features along with 10(slope) feature. This observation does not entirely support the claim 4.\relax }}{12}{figure.caption.12}}
\newlabel{fig:t2c-Heart}{{7}{12}{Explanation for corrupted features on UCI Heart Disease dataset. Left: Visualization of the TGT explanations on the modified dataset. We modified the 6(restecg) and 8(exang). However, the TGT recovers modifications in features 2(cp), 5(fbs), and 10(slope) instead. Right: Visualization of the TGT explanations with scVIS retrained on the modified dataset. With retrained scVIS model, TGT recovers the modified features along with 10(slope) feature. This observation does not entirely support the claim 4.\relax }{figure.caption.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Synthetic data}{13}{subsection.7.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces a) Synthetic data b) Synthetic data with the modification applied. We modify the data from group 1 across the $0^{th}$ dimension by $ax_{1}^{0} + b$. Here a and b are 2.0, 0.60 respectively.\relax }}{13}{figure.caption.13}}
\newlabel{fig:synth_modified}{{8}{13}{a) Synthetic data b) Synthetic data with the modification applied. We modify the data from group 1 across the $0^{th}$ dimension by $ax_{1}^{0} + b$. Here a and b are 2.0, 0.60 respectively.\relax }{figure.caption.13}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces We compare the explanations from the TGT algorithm (left) and the TGT with scaling extension algorithm(right) on the modified synthetic data. We can observe that the TGT with scaling extension has better correctness, and is able to identify the scaling we have applied across the first dimension (i.e. k=0). The $\gamma $ for this dimension is 0.87, which means the scaling factor is $e^{\gamma } \approx 2.38$. Moreover, the translation parameters are approximately same in both the variants of the TGT. \relax }}{13}{figure.caption.14}}
\newlabel{fig:synth_modified_change}{{9}{13}{We compare the explanations from the TGT algorithm (left) and the TGT with scaling extension algorithm(right) on the modified synthetic data. We can observe that the TGT with scaling extension has better correctness, and is able to identify the scaling we have applied across the first dimension (i.e. k=0). The $\gamma $ for this dimension is 0.87, which means the scaling factor is $e^{\gamma } \approx 2.38$. Moreover, the translation parameters are approximately same in both the variants of the TGT. \relax }{figure.caption.14}{}}
\abx@aux@defaultrefcontext{0}{plumb2020explaining}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{paszke2019pytorch}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bishop:2006:PRML}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{xie2018survey}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{belinkov-etal-2020-interpretability}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{verma2020counterfactual}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{carvalho2019machine}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{van2008visualizing}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{scvis}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bipolar}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{abadi2016tensorflow}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kingma2014autoencoding}{none/global//global/global}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Explanations between different groups for the Pima Indians Diabetes Database.\relax }}{14}{figure.caption.15}}
\newlabel{fig:diabetes_0}{{10}{14}{Explanations between different groups for the Pima Indians Diabetes Database.\relax }{figure.caption.15}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Probing Classifier and Feature Importance}{14}{subsection.7.3}}
\newlabel{app:dc}{{7.3}{14}{Probing Classifier and Feature Importance}{subsection.7.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Feature Importance by the binary classifier for the Pima Indians Diabetes Database. a) (top-left): Feature importance for the classifier between groups 0 and 1. b) (top-right): Feature importance for the classifier between groups 0 and 2. c) (bottom-left): Feature importance for the classifier between groups 1 and 2. We note that the classifiers give significant feature importances to the features which correspond to the deltas (refer fig. \ref  {fig:diabetes_0}).\relax }}{15}{figure.caption.16}}
\newlabel{fig:diabetes_1}{{11}{15}{Feature Importance by the binary classifier for the Pima Indians Diabetes Database. a) (top-left): Feature importance for the classifier between groups 0 and 1. b) (top-right): Feature importance for the classifier between groups 0 and 2. c) (bottom-left): Feature importance for the classifier between groups 1 and 2. We note that the classifiers give significant feature importances to the features which correspond to the deltas (refer fig. \ref {fig:diabetes_0}).\relax }{figure.caption.16}{}}
