\section{Ground truth explanations for Graph Explanations} \label{sec6}
For the evaluation of the PGExplainer the authors made use of predefined ground-truth explanations. These explanations are made possible by the use of synthetic datasets, generated based on the notion of motifs. In this section we express some concerns with regards to the use of motifs for generating ground-truth explanations. 

\paragraph{No ground-truth outside motif}
In the case of the node classification datasets the definition of the ground-truth explanation is only valid for a small number of nodes within a graph; those within a motif. In essence, for nodes outside the motifs, the ground-truth explanation is an empty graph---i.e. all surrounding edges have to be excluded from the explanation to achieve the maximum score. The same is true for non-mutagenic graphs in the Mutagenicity dataset. This is incompatible with the PGExplainers approach to determine its explanation. An empty graph can never produce the same explanation as the original graph, hence it will never be the explanation provided by the PGExplainer. 

The authors overcome this issue by excluding all nodes outside the motifs from their quantitative evaluation. However, this reduces the explanation task of the node classification datasets to a much simpler problem. For nodes outside the motif, the explanation has to be based on the absence instead of presence of edges. Solving these issues satisfactorily would require a new definition for the ground-truths for graph datasets. For example, in the case of the tree-cycle dataset, one could define the ground-truth of a node outside a motif to be the entire 7-hop subgraph as this would be the minimal number of steps to take before one can conclude that no cycles have been formed. We, however, believe this to be outside the scope of this replication.


\paragraph{Qualitative evaluation dependent on knowing size of motif}
The PGExplainer gives as output a mask that describes for each edge in the graph the probability of it being important for the models classification decision. To turn this into a visualizable explanations the top-$k$ edges are selected from each mask, i.e only the $k$ edges that have the highest influence on the models classification decision are considered part of the explanation. As a result, $k$ is a crucial hyperparameter for obtaining a visual explanation. If $k$ is set too high, the explanation could contain edges that actually only contribute to the final decision marginally. If $k$ is set too low, the explanation could be missing important parts of the graph. This difference in visual explanation quality was also empirically observed in the difference between the original and our explanations for the Mutagenicity dataset. 

As mentioned in the experimental setup of the qualitative evaluation, the authors, and preceding works, set the value of $k$ in the evaluation based on the amount of edges in the defining motif. However, this is not a possibility outside of the synthetic evaluation datasets. Hence, for real world applicability of the proposed explanation method a different approach has to be found to find $k$. For this reason, we believe that evaluating the quality of the explanations based using $k$ preset to the number of edges in the synthetic dataset is an aspect to reconsider. 

In essence, both the $k$-parameter and the earlier mentioned number of edges selected for the ground-truth can be considered as hyperparameters for the evaluation pipeline. By selecting a specific value for these parameters the evaluation can become biased towards assigning high credibility to explanations that have a specific characteristic. By performing an extensive search over these hyperparameters the results of the explanation evaluation can potentially be improved. In Sec.\,\ref{sec:extended_replication} of the appendix we present a short study on how these hyperparameters can influence the final results of the evaluation. 