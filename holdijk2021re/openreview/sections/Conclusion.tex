\section{Conclusion}\label{sec:conclusion}
In this work, we have presented a replication of the paper \textit{Parameterized Explainer for Graph Neural Network}. The replication experiments have lead us to a number of conclusions. First, based on the paper alone, it is difficult to replicate the presented results. The main contributing factor is the discrepancy between the provided details in the paper and those in the codebase. Based on communication with the authors, we conclude that the hyperparameter settings presented in the paper are oversimplified. For the method to work, more hyperparameter tuning is needed then the paper suggests. This is validated by our ablation study.

Second, even with the provided codebase, replication of the presented results is still arduous. With the configurations pulled from the codebase used in our re-evaluation, we still found lackluster results for a number of the datasets. We accredit this problem mainly to the structure of the codebase itself. The code is overly convoluted with the experiment configurations being overridden in numerous locations. Due to this, it is unclear if the configurations we found in the codebase are those that generated the results presented in the original paper. 

Lastly, as discussed in Sec.\,\ref{sec6}, we are uncertain if the evaluation based on synthetic datasets as used in the evaluation is valid. However, we can not contribute this issue to only the authors' paper as it is also used in other graph explanation papers, including the GNNExplainer. In addition to showing that these issues exists, our extended evaluation presented in appendix Sec\,\ref{sec:extended_replication} showed that it is not trivial to solve them based on the current definition of a ground-truth explanation for motif graphs. Rethinking the evaluation for Graph Neural Networks Explainers is therefore important future work. 

% \subsection{Discussion replication THESE ALL HAVE TO BE MERGED}
% Based on the paper alone it is impossible to replicate the results presented in the paper, even if the complications of the evaluation itself are ignored. However, as the results presented above show, even with the provided codebase replicating the presented results is still not possible. First, the code base is badly structured and does not contain a central place describing the used configurations for training the models. Because of this a number of things remained unclear about the configuration of the models that might have lead to the reduced accuracy. For example, the training script configuration contained the possibility to use weight sharing and dropout to prevent over-fitting, but this is not used in any of the models. We hypothesize that this might be used for improving the accuracy of the BA-community model. 

% The replicated quantitative, qualitative and efficiency experiments similarly show that it is hard to reproduce the results prevented in the paper. Again, the main configuration files required to faithfully replicate the main results are missing. We believe that this effect is worsened by the crucial role of badly documented hyper parameters such as the entropy and size regularization coefficients. In the qualitative evaluation we expect that the importance of these hyper-parameters is hidden by the handpicked number of edges show in the explanation. The effect of these parameters will be further explored in the extended reproduction.

% In this work we presented both a replication and an extended reproduction of the paper \textit{Parameterized Explainer for Graph Neural Network}. In short, the work shows that the main claims of the paper are valid. The proposed PGExplainer significantly outperforms previous work in the form of the GNNExplainer both in accuracy and efficiency. This remains true even with an updated evaluation pipeline during the extended reproduction. 

% \subsection{Discussion MERGE ASWELL}
% The results presented in the extended reproduction seem to be in line with the claims related to the GNNExplainer by the authors of the PGExplainer. Both in terms of accuracy and efficiency, the PGExplainer outperforms the GNNExplainer by a significant margin. However, the handling of ground-truth explanations remains inconsistent for the presented datasets. While the improvements in the evaluation pipeline, such as not using the training set for evaluation, might have improved the validity of the claims the usefulness of the used ground-truth explanations remain questionable. 

% It must however be noted that it would not have been possible to reach this conclusion based on the details presented in the paper. While performing the replication a large number of inconsistencies between the paper and the code base were found. This includes some major oversights by the author such as keeping batch-normalization in training mode during evaluation. These oversights were confirmed by the authors. In addition to this, based on the results presented in the replication experiment alone, we would not feel confident to validate the claims of the authors. The originally used evaluation pipeline makes to much questionable assumptions to fully trust these results. The updated evaluation pipeline of the extended reproduction however does provide this confidence, we believe. 

% Nevertheless, the question remains if the used ground-truth explanations for the synthetic datasets are valid. We can however not contribute this issue to only this paper as it is also used in other graph explanation papers, including the GNNExplainer. Validating the use of synthetic datasets using motifs for explaining graph neural network is important future work. 


