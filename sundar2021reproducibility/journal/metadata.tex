% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/varun19299/rigl-reproducibility}
\def \codeDOI{}
\def \codeSWH{swh:1:dir:0707870fafa16ef60dc64071e1aed482e373e75f}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{}
\def \editorORCID{}
\def \reviewerINAME{Anonymous Reviewers}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{01 November 2018}
\def \dateACCEPTED{}
\def \datePUBLISHED{}
\def \articleTITLE{[Re] Rigging the Lottery: Making All Tickets Winner}
\def \articleTYPE{Editorial}
\def \articleDOMAIN{ML Reproducibility Challenge 2020}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2021}
\def \reviewURL{https://openreview.net/forum?id=riCIeP6LzEE}
\def \articleABSTRACT{RigL , a sparse training algorithm, claims to directly train sparse networks that match or exceed the performance of existing dense-to-sparse training techniques (such as pruning) for a fixed parameter count and compute budget. We implement RigL from scratch in Pytorch and reproduce its performance on CIFAR-10 within 0.1% of the reported value. On both CIFAR-10/100, the central claim holds---given a fixed training budget, RigL surpasses existing dynamic-sparse training methods over a range of target sparsities. By training longer, the performance can match or exceed iterative pruning, while consuming constant FLOPs throughout training. We also show that there is little benefit in tuning RigL's hyper-parameters for every sparsity, initialization pair---the reference choice of hyperparameters is often close to optimal performance. Going beyond the original paper, we find that the optimal initialization scheme depends on the training constraint. While the Erdos-Renyi-Kernel distribution outperforms the Uniform distribution for a fixed parameter count, for a fixed FLOP count, the latter performs better. Finally, redistributing layer-wise sparsity while training can bridge the performance gap between the two initialization schemes, but increases computational cost.}
\def \replicationCITE{Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In Proceedings of Machine Learning and Systems (ICML), July 2020.}
\def \replicationBIB{rigl}
\def \replicationURL{https://arxiv.org/pdf/1911.11134.pdf}
\def \replicationDOI{}
\def \contactNAME{Varun Sundar}
\def \contactEMAIL{vsundar4@wisc.edu}
\def \articleKEYWORDS{rescience c, rescience x, python, pytorch, sparse-networks}
\def \journalNAME{ReScience C}
\def \journalVOLUME{4}
\def \journalISSUE{1}
\def \articleNUMBER{}
\def \articleDOI{}
\def \authorsFULL{Varun Sundar and Rajat Vadiraj Dwaraknath}
\def \authorsABBRV{V. Sundar and R.V. Dwaraknath}
\def \authorsSHORT{Sundar and Dwaraknath}
\title{\articleTITLE}
\date{}
\author[1,\orcid{0000-0002-8261-3630}]{Varun Sundar}
\author[2,,\orcid{0000-0002-6972-589X}]{Rajat Vadiraj Dwaraknath}
\affil[1]{University of Wisconsin Madison, Wisconsin, USA}
\affil[2]{Synchrotron SOLEIL, Division Exp√©riences, Gif sur Yvette, France}
