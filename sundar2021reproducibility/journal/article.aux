\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@refcontext{none/global//global/global}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\babel@aux{american}{}
\pgfsyspdfmark {pgfid1}{8391059}{49131591}
\newmarginnote{note.1.1}{{1}{8391059sp}}
\abx@aux@cite{han2016eie}
\abx@aux@segm{0}{0}{han2016eie}
\abx@aux@cite{ashbyexploiting}
\abx@aux@segm{0}{0}{ashbyexploiting}
\abx@aux@cite{Srinivas_2017_CVPR_Workshops}
\abx@aux@segm{0}{0}{Srinivas_2017_CVPR_Workshops}
\abx@aux@cite{han2016eie}
\abx@aux@segm{0}{0}{han2016eie}
\abx@aux@cite{ashbyexploiting}
\abx@aux@segm{0}{0}{ashbyexploiting}
\abx@aux@cite{Srinivas_2017_CVPR_Workshops}
\abx@aux@segm{0}{0}{Srinivas_2017_CVPR_Workshops}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{to_prune_or_not}
\abx@aux@segm{0}{0}{to_prune_or_not}
\abx@aux@cite{to_prune_or_not}
\abx@aux@segm{0}{0}{to_prune_or_not}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Scope of reproducibility}{2}{section.2}\protected@file@percent }
\newlabel{sec:claims}{{2}{2}{Scope of reproducibility}{section.2}{}}
\abx@aux@cite{abadi2016tensorflow}
\abx@aux@segm{0}{0}{abadi2016tensorflow}
\abx@aux@cite{abadi2016tensorflow}
\abx@aux@segm{0}{0}{abadi2016tensorflow}
\abx@aux@cite{Pytorch}
\abx@aux@segm{0}{0}{Pytorch}
\abx@aux@cite{Pytorch}
\abx@aux@segm{0}{0}{Pytorch}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{wandb}
\abx@aux@segm{0}{0}{wandb}
\abx@aux@cite{wandb}
\abx@aux@segm{0}{0}{wandb}
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Mask Initialization}{3}{paragraph*.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Mask Updates}{3}{paragraph*.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Pruning Strategy}{3}{paragraph*.3}\protected@file@percent }
\abx@aux@cite{Krizhevsky09learningmultiple}
\abx@aux@segm{0}{0}{Krizhevsky09learningmultiple}
\abx@aux@cite{Krizhevsky09learningmultiple}
\abx@aux@segm{0}{0}{Krizhevsky09learningmultiple}
\abx@aux@cite{Wide_ResNet_BMVC2016_87}
\abx@aux@segm{0}{0}{Wide_ResNet_BMVC2016_87}
\abx@aux@cite{Wide_ResNet_BMVC2016_87}
\abx@aux@segm{0}{0}{Wide_ResNet_BMVC2016_87}
\abx@aux@cite{Krizhevsky09learningmultiple}
\abx@aux@segm{0}{0}{Krizhevsky09learningmultiple}
\abx@aux@cite{Krizhevsky09learningmultiple}
\abx@aux@segm{0}{0}{Krizhevsky09learningmultiple}
\abx@aux@cite{He_2016_CVPR}
\abx@aux@segm{0}{0}{He_2016_CVPR}
\abx@aux@cite{He_2016_CVPR}
\abx@aux@segm{0}{0}{He_2016_CVPR}
\abx@aux@cite{goyal2017accurate}
\abx@aux@segm{0}{0}{goyal2017accurate}
\abx@aux@cite{goyal2017accurate}
\abx@aux@segm{0}{0}{goyal2017accurate}
\abx@aux@cite{ioffe2015batch}
\abx@aux@segm{0}{0}{ioffe2015batch}
\abx@aux@cite{ioffe2015batch}
\abx@aux@segm{0}{0}{ioffe2015batch}
\abx@aux@cite{He_2019_CVPR}
\abx@aux@segm{0}{0}{He_2019_CVPR}
\abx@aux@cite{He_2019_CVPR}
\abx@aux@segm{0}{0}{He_2019_CVPR}
\abx@aux@cite{optuna_2019}
\abx@aux@segm{0}{0}{optuna_2019}
\abx@aux@cite{optuna_2019}
\abx@aux@segm{0}{0}{optuna_2019}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Test accuracy of reference and our implementations on CIFAR-10,} tabulated for three different sparsities. Note that the runs listed here do not use a separate validation set while training.\relax }}{4}{table.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:replication_verify}{{1}{4}{\textbf {Test accuracy of reference and our implementations on CIFAR-10,} tabulated for three different sparsities. Note that the runs listed here do not use a separate validation set while training.\relax }{table.caption.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Growth Strategy}{4}{paragraph*.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Settings}{4}{section.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model descriptions}{4}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Datasets and Training descriptions}{4}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Hyperparameters}{4}{subsection.4.3}\protected@file@percent }
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{Mocanu2018SET}
\abx@aux@segm{0}{0}{Mocanu2018SET}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{to_prune_or_not}
\abx@aux@segm{0}{0}{to_prune_or_not}
\abx@aux@cite{to_prune_or_not}
\abx@aux@segm{0}{0}{to_prune_or_not}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Baseline implementations}{5}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Computational requirements}{5}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{5}{section.5}\protected@file@percent }
\newlabel{sec:results}{{5}{5}{Results}{section.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}WideResNet-22 on CIFAR-10}{5}{subsection.5.1}\protected@file@percent }
\newlabel{cifar-10-results}{{5.1}{5}{WideResNet-22 on CIFAR-10}{subsection.5.1}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {WideResNet-22-2 on CIFAR10}, tabulated for two density $(1-s)$ values. We group methods by their FLOP requirement and in each group, we mark the best accuracy in bold. Similar to \citeauthor {rigl}\supercite {rigl}, we assume that algorithms utilize sparsity during training. All results are obtained by methods implemented in our unified codebase.\relax }}{6}{table.caption.6}\protected@file@percent }
\newlabel{tab:cifar10-main-results}{{2}{6}{\textbf {WideResNet-22-2 on CIFAR10}, tabulated for two density $(1-s)$ values. We group methods by their FLOP requirement and in each group, we mark the best accuracy in bold. Similar to \citet {rigl}, we assume that algorithms utilize sparsity during training. All results are obtained by methods implemented in our unified codebase.\relax }{table.caption.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Test Accuracy vs Sparsity on CIFAR-10,} plotted for Random initialization \textbf  {(left)}, ERK initialization \textbf  {(center)}, and for training $2\times $ longer \textbf  {(right)}. Owing to random growth, SET can be unstable when training for longer durations with higher sparsities. Overall, \textit  {RigL}\textsubscript  {$2 \times $} (ERK) achieves highest test accuracy.\relax }}{6}{figure.caption.7}\protected@file@percent }
\newlabel{fig:cifar10-main-results}{{1}{6}{\textbf {Test Accuracy vs Sparsity on CIFAR-10,} plotted for Random initialization \textbf {(left)}, ERK initialization \textbf {(center)}, and for training $2\times $ longer \textbf {(right)}. Owing to random growth, SET can be unstable when training for longer durations with higher sparsities. Overall, \textit {RigL}\textsubscript {$2 \times $} (ERK) achieves highest test accuracy.\relax }{figure.caption.7}{}}
\abx@aux@cite{frankle2018lottery}
\abx@aux@segm{0}{0}{frankle2018lottery}
\abx@aux@cite{frankle2018lottery}
\abx@aux@segm{0}{0}{frankle2018lottery}
\abx@aux@cite{TPE_Bergstra}
\abx@aux@segm{0}{0}{TPE_Bergstra}
\abx@aux@cite{TPE_Bergstra}
\abx@aux@segm{0}{0}{TPE_Bergstra}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}ResNet-50 on CIFAR100}{7}{subsection.5.2}\protected@file@percent }
\newlabel{cifar-100-results}{{5.2}{7}{ResNet-50 on CIFAR100}{subsection.5.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces entry for figure}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:cifar100-main-results}{{2}{7}{entry for figure}{figure.2}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Benchmarking sparse ResNet-50s on CIFAR-100,} tabulated by performance and cost \textbf  {(below)}, and plotted across densities \textbf  {(right)}. In each group below, \textit  {RigL} outperforms or matches existing sparse-to-sparse and dense-to-sparse methods. Notably, \textit  {RigL}\textsubscript  {$3\times $} at 90\% sparsity and \textit  {RigL}\textsubscript  {$2\times $} at 80\% sparsity surpass iterative pruning with similar FLOP consumption. \textit  {RigL}\textsubscript  {$2\times $} (ERK) further improves performance but requires a larger training budget. \relax }}{7}{table.3}\protected@file@percent }
\newlabel{tab:cifar100-main-results}{{3}{7}{\textbf {Benchmarking sparse ResNet-50s on CIFAR-100,} tabulated by performance and cost \textbf {(below)}, and plotted across densities \textbf {(right)}. In each group below, \textit {RigL} outperforms or matches existing sparse-to-sparse and dense-to-sparse methods. Notably, \textit {RigL}\textsubscript {$3\times $} at 90\% sparsity and \textit {RigL}\textsubscript {$2\times $} at 80\% sparsity surpass iterative pruning with similar FLOP consumption. \textit {RigL}\textsubscript {$2\times $} (ERK) further improves performance but requires a larger training budget. \relax }{table.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Hyperparameter Tuning}{7}{subsection.5.3}\protected@file@percent }
\newlabel{hyperparameter-tuning}{{5.3}{7}{Hyperparameter Tuning}{subsection.5.3}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {Reference vs Optimal $(\alpha , \Delta T)$ on CIFAR-10.} Optimal hyperparameters are obtained by tuning with a TPE sampler in Optuna. The difference between the reference and optimal performance is small, indicating that there is not a significant benefit in tuning $(\alpha , \Delta T)$ individually for each initialization and sparsity configuration.\relax }}{7}{table.caption.8}\protected@file@percent }
\newlabel{tab:effect-alpha-deltaT}{{4}{7}{\textbf {Reference vs Optimal $(\alpha , \Delta T)$ on CIFAR-10.} Optimal hyperparameters are obtained by tuning with a TPE sampler in Optuna. The difference between the reference and optimal performance is small, indicating that there is not a significant benefit in tuning $(\alpha , \Delta T)$ individually for each initialization and sparsity configuration.\relax }{table.caption.8}{}}
\newlabel{tab:replication_verify}{{4}{7}{\textbf {Reference vs Optimal $(\alpha , \Delta T)$ on CIFAR-10.} Optimal hyperparameters are obtained by tuning with a TPE sampler in Optuna. The difference between the reference and optimal performance is small, indicating that there is not a significant benefit in tuning $(\alpha , \Delta T)$ individually for each initialization and sparsity configuration.\relax }{table.caption.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{$(\alpha , \Delta T)$ vs Sparsities}{7}{paragraph*.9}\protected@file@percent }
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\abx@aux@cite{dettmers2020sparse}
\abx@aux@segm{0}{0}{dettmers2020sparse}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Learning Rate vs Sparsities}{8}{paragraph*.11}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Results beyond Original Paper}{8}{section.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Sparsity Distribution vs FLOP Consumption}{8}{subsection.6.1}\protected@file@percent }
\newlabel{effect-sparsity-distribution}{{6.1}{8}{Sparsity Distribution vs FLOP Consumption}{subsection.6.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Learning Rate vs Sparsity on CIFAR-10.} Runs using a learning rate $> 0.1$ do not converge and are not plotted here. There is little benefit in tuning the learning rate for each sparsity, and $0.1, 0.05$ are good choices overall.\relax }}{8}{figure.caption.10}\protected@file@percent }
\newlabel{fig:lr-sweep}{{3}{8}{\textbf {Learning Rate vs Sparsity on CIFAR-10.} Runs using a learning rate $> 0.1$ do not converge and are not plotted here. There is little benefit in tuning the learning rate for each sparsity, and $0.1, 0.05$ are good choices overall.\relax }{figure.caption.10}{}}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\abx@aux@cite{rigl}
\abx@aux@segm{0}{0}{rigl}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Test Accuracy vs FLOP consumption of WideResNet-22-2 on CIFAR-10 and ResNet-50 on CIFAR-100,} compared for Random and ERK initializations. For the same FLOP budget, models trained with ERK initialization must be more sparse, resulting in inferior performance.\relax }}{9}{figure.caption.12}\protected@file@percent }
\newlabel{fig:erk-vs-random-FLOPs}{{4}{9}{\textbf {Test Accuracy vs FLOP consumption of WideResNet-22-2 on CIFAR-10 and ResNet-50 on CIFAR-100,} compared for Random and ERK initializations. For the same FLOP budget, models trained with ERK initialization must be more sparse, resulting in inferior performance.\relax }{figure.caption.12}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textbf  {Effect of redistribution during \textit  {RigL} updates, evaluated on CIFAR10 and CIFAR100}. By utilising sparse gradient or sparse momentum based redistribution, \textit  {RigL} (Random) matches \textit  {RigL} (ERK)'s performance. Among Random and ERK initialized experiments, we mark the best metrics under each sparsity and dataset in bold. \relax }}{9}{table.caption.13}\protected@file@percent }
\newlabel{tab:effect-redistribution}{{5}{9}{\textbf {Effect of redistribution during \textit {RigL} updates, evaluated on CIFAR10 and CIFAR100}. By utilising sparse gradient or sparse momentum based redistribution, \textit {RigL} (Random) matches \textit {RigL} (ERK)'s performance. Among Random and ERK initialized experiments, we mark the best metrics under each sparsity and dataset in bold. \relax }{table.caption.13}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Effect of Redistribution}{9}{subsection.6.2}\protected@file@percent }
\newlabel{effect-redistribution}{{6.2}{9}{Effect of Redistribution}{subsection.6.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{9}{section.7}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Effect of redistribution on \textit  {RigL}'s performance,} evaluated using WideResNet-22-2 on CIFAR10 at 80\% sparsity. \textbf  {(left)} FLOPs required per forward pass, shown relative to the dense baseline, rises quickly and saturates within a few epochs (\textasciitilde 10k steps) for both sparse gradient and sparse momentum based redistribution. \textbf  {(right)} Comparison of the final density distribution against Random and ERK counterparts. ``b'' refers to block and ``l'' layer here.\relax }}{10}{figure.caption.14}\protected@file@percent }
\newlabel{fig:density-dist-evolution}{{5}{10}{\textbf {Effect of redistribution on \textit {RigL}'s performance,} evaluated using WideResNet-22-2 on CIFAR10 at 80\% sparsity. \textbf {(left)} FLOPs required per forward pass, shown relative to the dense baseline, rises quickly and saturates within a few epochs (\textasciitilde 10k steps) for both sparse gradient and sparse momentum based redistribution. \textbf {(right)} Comparison of the final density distribution against Random and ERK counterparts. ``b'' refers to block and ``l'' layer here.\relax }{figure.caption.14}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{What was easy}{10}{paragraph*.15}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{What was difficult}{10}{paragraph*.16}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Communication with original authors}{10}{paragraph*.17}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{29E25303DA6CE5DF62CF9D951846E692}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{han2016eie}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ashbyexploiting}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Srinivas_2017_CVPR_Workshops}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rigl}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Mocanu2018SET}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{dettmers2020sparse}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{to_prune_or_not}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{abadi2016tensorflow}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Pytorch}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{wandb}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Krizhevsky09learningmultiple}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Wide_ResNet_BMVC2016_87}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{He_2016_CVPR}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{goyal2017accurate}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ioffe2015batch}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{He_2019_CVPR}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{optuna_2019}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{frankle2018lottery}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{TPE_Bergstra}{none/global//global/global}
\gdef \@abspage@last{11}
