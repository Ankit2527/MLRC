% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/shin-ee-chen/UvA_FACT_2021}
\def \codeDOI{}
\def \codeSWH{swh:1:dir:9d960937f2b11bb67b53bbe4eaa940c14cadf9d1}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{Koustuv Sinha}
\def \editorORCID{}
\def \reviewerINAME{Anonymous Reviewers}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{29 January 2021}
\def \dateACCEPTED{01 April 2021}
\def \datePUBLISHED{26 May 2021}
\def \articleTITLE{[Re] Replication Study of 'Generative causal explanations of black-box classifiers'}
\def \articleTYPE{Replication}
\def \articleDOMAIN{ML Reproducibility Challenge 2020}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2021}
\def \reviewURL{https://openreview.net/forum?id=Vqtf4kZg2j}
\def \articleABSTRACT{Recent technological advances have provided access to an unprecedented volume of data, which has been historically under-exploited. To overcome this, artificial intelligence methods (specifically, deep learning) are being granted more and more decision-making authority. However, while achieving state-of-the-art performance, most such algorithms are considered black boxes. The implication is that machine learning methods are rarely checked and interpretability is lost in favour of effectiveness. This paper seeks to reproduce and replicate the findings of the paper “Generative causal explanations of black-box classifiers”; a recent deep generative modelling approach that allows for interpretable understanding of black-box classifiers. Specifically, their contribution is a decorrelated low-dimensional latent space learned by exploiting the results of an accessible but uninterpretable classifier. We measure the validity of their approach through a series of experiments to check the accuracy of their claims and effectiveness of their approach. While this paper shows full replication is possible, with the core idea being elegant and easy to implement, severe issues in their approach are brought to light when extending their framework to a new domain.}
\def \replicationCITE{O'Shaughnessy, M., Canal, G., Connor, M., Davenport, M., & Rozell, C. (2020). Generative causal explanations of black-box classifiers. arXiv preprint arXiv:2006.13913.}
\def \replicationBIB{oshaughnessy2020generative}
\def \replicationURL{https://arxiv.org/abs/2006.13913}
\def \replicationDOI{}
\def \contactNAME{Ivo Verhoeven}
\def \contactEMAIL{ivo.verhoeven@student.uva.nl}
\def \articleKEYWORDS{rescience c, rescience x, python, causal explanations, variational  autoencoders,  explainable  artificial  intelligence, post-hoc explanation}
\def \journalNAME{ReScience C}
\def \journalVOLUME{7}
\def \journalISSUE{2}
\def \articleNUMBER{}
\def \articleDOI{}
\def \authorsFULL{Ivo Verhoeven et al.}
\def \authorsABBRV{I. Verhoeven et al.}
\def \authorsSHORT{Verhoeven et al.}
\title{\articleTITLE}
\date{}
\author[1,\orcid{0000-0002-5163-210X}]{Ivo Verhoeven}
\author[1,\orcid{0000-0001-8757-344X}]{Xinyi Chen}
\author[1,\orcid{0000-0001-7419-2031}]{Qingzhi Hu}
\author[1,\orcid{0000-0001-6922-2443}]{Mario Holubar}
\affil[1]{University of Amsterdam, Amsterdam, the Netherlands}
